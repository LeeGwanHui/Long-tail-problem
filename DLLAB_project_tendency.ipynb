{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be564ffe-7c6a-4983-a927-c53f79fe530b",
   "metadata": {},
   "source": [
    "> ### EEE4423: Deep Learning Lab\n",
    "\n",
    "# Final Project: Long-tail Visual Recognition for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d8d4f-d166-4203-9223-f5b675b2ebd8",
   "metadata": {},
   "source": [
    "<h4><div style=\"text-align: right\"> Due date: June 24, 2022.  </div> <br>\n",
    "<div style=\"text-align: right\"> Please upload your file @ LearnUs and submit via e-mail by 2 PM in the form of [ID_Name_project.ipynb]. </div></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c6334f",
   "metadata": {},
   "source": [
    "이 파일은 정확도의 경향성을 보기 위한 파일이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0d7d3b2-32a4-4808-9476-edc89d829705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from misc.project.utils import resnet18, IMBALANCECIFAR10, IMBALANCECIFAR100, compute_accuracy\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) ## 경고 무시\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6af980f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6: INSTANTIATE OPTIMIZER CLASS\n"
     ]
    }
   ],
   "source": [
    "print('STEP 6: INSTANTIATE OPTIMIZER CLASS')\n",
    "\n",
    "DATASET = 'CIFAR10' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'exp' #['exp', 'step'] # 질문\n",
    "IMB_FACTOR = 0.1 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/featuresampling/CIFAR10/exp-0.1' \n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128 \n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 2e-4\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebfa934f-3869-4519-86e1-abd9173b62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "665d1d9f-ea6a-4d00-bd1e-e42d28565546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: LOADING DATASET\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[5000, 3871, 2997, 2320, 1796, 1391, 1077, 834, 645, 500]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print('STEP 1: LOADING DATASET')\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "if DATASET == 'CIFAR10':\n",
    "    train_dataset_0 = IMBALANCECIFAR10(root='../dataset/project', imb_type=IMB_TYPE, imb_factor=IMB_FACTOR, train=True, download=True, transform=transform_train)\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root='../dataset/project', train=False, download=True, transform=transform_test)\n",
    "elif DATASET == 'CIFAR100':\n",
    "    train_dataset_0 = IMBALANCECIFAR100(root='../dataset/project', imb_type=IMB_TYPE, imb_factor=IMB_FACTOR, train=True, download=True, transform=transform_train)\n",
    "    test_dataset = torchvision.datasets.CIFAR100(root='../dataset/project', train=False, download=True, transform=transform_test)\n",
    "\n",
    "cls_num_list = train_dataset_0.get_cls_num_list() # 각 class에 몇개가 들어있는지 나타냄 \n",
    "print('cls num list:')\n",
    "print(cls_num_list)\n",
    "num_classes = len(cls_num_list)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ca6d70a-b8d0-41cd-87d6-524874276249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20431\n",
      "18388\n",
      "2043\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "torch.manual_seed(torch.initial_seed())\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(0)\n",
    "dataset_size = len(train_dataset_0)\n",
    "validation_size = int(dataset_size * 0.1) if not int(dataset_size * 0.1)==0 else 1 \n",
    "train_size = dataset_size - validation_size\n",
    "test_size = 0\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset_0 = random_split(train_dataset_0, [train_size, validation_size, test_size],generator=generator)\n",
    "\n",
    "print(len(train_dataset_0))\n",
    "print(len(train_dataset))\n",
    "print(len(validation_dataset))\n",
    "print(len(test_dataset_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80109161",
   "metadata": {},
   "source": [
    "여기서 진행하는 것은 re-sampling이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f82fd818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data sampler 구현 \n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import pdb\n",
    "\n",
    "# data 중에 random하게 하나를 뽑음.\n",
    "class RandomCycleIter:\n",
    "    def __init__ (self, data, test_mode=False):\n",
    "        self.data_list = list(data)\n",
    "        self.length = len(self.data_list)\n",
    "        self.i = self.length - 1\n",
    "        self.test_mode = test_mode\n",
    "    def __iter__ (self):\n",
    "        return self\n",
    "    def __next__ (self):\n",
    "        self.i += 1\n",
    "        if self.i == self.length:\n",
    "            self.i = 0\n",
    "            if not self.test_mode:\n",
    "                random.shuffle(self.data_list)\n",
    "        return self.data_list[self.i]\n",
    "    \n",
    "def class_aware_sample_generator (cls_iter, data_iter_list, n, num_samples_cls=1):\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < n:\n",
    "        if j >= num_samples_cls:\n",
    "            j = 0\n",
    "        if j == 0:\n",
    "            temp_tuple = next(zip(*[data_iter_list[next(cls_iter)]]*num_samples_cls))\n",
    "            yield temp_tuple[j]\n",
    "        else:\n",
    "            yield temp_tuple[j]\n",
    "        i += 1\n",
    "        j += 1\n",
    "\n",
    "class ClassAwareSampler (Sampler):\n",
    "    def __init__(self, data_source, num_samples_cls=1,):\n",
    "        labels = []\n",
    "        for i in range(len(train_dataset)) :\n",
    "            labels.append(train_dataset[i][1])\n",
    "        num_classes = 10\n",
    "        self.class_iter = RandomCycleIter(range(num_classes))\n",
    "        cls_data_list = [list() for _ in range(num_classes)] # [[], [], [], [], [], [], [], [], [], []]\n",
    "        for i, label in enumerate(labels):\n",
    "            cls_data_list[label].append(i)\n",
    "        self.data_iter_list = [RandomCycleIter(x) for x in cls_data_list]\n",
    "        # print(self.data_iter_list)\n",
    "        self.num_samples = max([len(x) for x in cls_data_list]) * len(cls_data_list)\n",
    "        self.num_samples_cls = num_samples_cls\n",
    "    def __iter__ (self):\n",
    "        return class_aware_sample_generator(self.class_iter, self.data_iter_list,\n",
    "                                            self.num_samples, self.num_samples_cls)\n",
    "    def __len__ (self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8716617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: MAKING DATASET ITERABLE\n"
     ]
    }
   ],
   "source": [
    "print('STEP 2: MAKING DATASET ITERABLE')\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=None,\n",
    "    num_workers=4, drop_last=False, sampler = ClassAwareSampler(train_dataset,4))\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    validation_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=4, drop_last=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=100, shuffle=False,\n",
    "    num_workers=4, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71e09a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=None,\n",
    "    num_workers=4, drop_last=False, sampler = ClassAwareSampler(train_dataset,4))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=100, shuffle=False,\n",
    "    num_workers=4, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90b9e350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000. 5000. 5000. 5000. 5000. 5000. 5000. 5000. 5000. 5000.]\n",
      "50000.0\n"
     ]
    }
   ],
   "source": [
    "# 다른 방법이긴하지만 이렇게 표현가능  \n",
    "list1 = np.zeros(10)\n",
    "for batch_index, data in enumerate(train_loader):\n",
    "    image, target = data\n",
    "    for j in range(len(target)) :\n",
    "        k =target[j] \n",
    "        list1[k]+=1\n",
    "\n",
    "print(list1)\n",
    "print(list1.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1a22399-4ce8-4320-ab5c-813fdfa045d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: CREATE MODEL CLASS (VGG16)\n"
     ]
    }
   ],
   "source": [
    "print('STEP 3: CREATE MODEL CLASS (VGG16)')\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.backbone = resnet18()\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone(x)\n",
    "        x = F.adaptive_max_pool2d(x, 1)\n",
    "        x = x.view(batch_size, -1)\n",
    "        pred = self.classifier(x)\n",
    "#         print(pred.size()) # torch.Size([128, 10])\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "afdb726e-f03a-4032-904f-deb21613086a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: INSTANTIATE MODEL CLASS\n",
      "STEP 5: INSTANTIATE LOSS CLASS\n"
     ]
    }
   ],
   "source": [
    "print('STEP 4: INSTANTIATE MODEL CLASS')\n",
    "\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "\n",
    "print('STEP 5: INSTANTIATE LOSS CLASS')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1828d09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea710170-226d-4093-84e0-cacbee35ed81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [001] \t Loss 2.0046 \t Acc 39.35 \t AccHead 38.36 \t AccTail 43.16\n",
      "Epoch: [002] \t Loss 1.4182 \t Acc 51.49 \t AccHead 53.18 \t AccTail 45.05\n",
      "Epoch: [003] \t Loss 1.2171 \t Acc 56.58 \t AccHead 55.71 \t AccTail 59.91\n",
      "Epoch: [004] \t Loss 1.0492 \t Acc 58.49 \t AccHead 57.01 \t AccTail 64.15\n",
      "Epoch: [005] \t Loss 0.9340 \t Acc 62.56 \t AccHead 62.45 \t AccTail 62.97\n",
      "Epoch: [006] \t Loss 0.8414 \t Acc 61.72 \t AccHead 62.38 \t AccTail 59.20\n",
      "Epoch: [007] \t Loss 0.7743 \t Acc 68.82 \t AccHead 70.66 \t AccTail 61.79\n",
      "Epoch: [008] \t Loss 0.7146 \t Acc 70.93 \t AccHead 74.12 \t AccTail 58.73\n",
      "Epoch: [009] \t Loss 0.6637 \t Acc 70.44 \t AccHead 70.41 \t AccTail 70.52\n",
      "Epoch: [010] \t Loss 0.6219 \t Acc 64.46 \t AccHead 62.75 \t AccTail 70.99\n",
      "Epoch: [011] \t Loss 0.5839 \t Acc 71.27 \t AccHead 72.58 \t AccTail 66.27\n",
      "Epoch: [012] \t Loss 0.5545 \t Acc 73.08 \t AccHead 73.69 \t AccTail 70.75\n",
      "Epoch: [013] \t Loss 0.5302 \t Acc 72.83 \t AccHead 73.38 \t AccTail 70.75\n",
      "Epoch: [014] \t Loss 0.5025 \t Acc 70.93 \t AccHead 71.65 \t AccTail 68.16\n",
      "Epoch: [015] \t Loss 0.4850 \t Acc 76.11 \t AccHead 78.26 \t AccTail 67.92\n",
      "Epoch: [016] \t Loss 0.4634 \t Acc 74.11 \t AccHead 75.54 \t AccTail 68.63\n",
      "Epoch: [017] \t Loss 0.4483 \t Acc 73.67 \t AccHead 73.93 \t AccTail 72.64\n",
      "Epoch: [018] \t Loss 0.4387 \t Acc 73.57 \t AccHead 74.43 \t AccTail 70.28\n",
      "Epoch: [019] \t Loss 0.4237 \t Acc 74.40 \t AccHead 76.47 \t AccTail 66.51\n",
      "Epoch: [020] \t Loss 0.4086 \t Acc 76.60 \t AccHead 79.37 \t AccTail 66.04\n",
      "Epoch: [021] \t Loss 0.3967 \t Acc 74.01 \t AccHead 76.22 \t AccTail 65.57\n",
      "Epoch: [022] \t Loss 0.3877 \t Acc 75.28 \t AccHead 76.03 \t AccTail 72.41\n",
      "Epoch: [023] \t Loss 0.3778 \t Acc 73.37 \t AccHead 74.12 \t AccTail 70.52\n",
      "Epoch: [024] \t Loss 0.3726 \t Acc 76.99 \t AccHead 79.93 \t AccTail 65.80\n",
      "Epoch: [025] \t Loss 0.3639 \t Acc 78.51 \t AccHead 81.10 \t AccTail 68.63\n",
      "Epoch: [026] \t Loss 0.3563 \t Acc 74.50 \t AccHead 74.92 \t AccTail 72.88\n",
      "Epoch: [027] \t Loss 0.3447 \t Acc 71.76 \t AccHead 73.26 \t AccTail 66.04\n",
      "Epoch: [028] \t Loss 0.3469 \t Acc 74.79 \t AccHead 76.03 \t AccTail 70.05\n",
      "Epoch: [029] \t Loss 0.3318 \t Acc 77.34 \t AccHead 79.43 \t AccTail 69.34\n",
      "Epoch: [030] \t Loss 0.3344 \t Acc 76.65 \t AccHead 79.74 \t AccTail 64.86\n",
      "Epoch: [031] \t Loss 0.3216 \t Acc 77.14 \t AccHead 78.94 \t AccTail 70.28\n",
      "Epoch: [032] \t Loss 0.3195 \t Acc 74.35 \t AccHead 76.59 \t AccTail 65.80\n",
      "Epoch: [033] \t Loss 0.3208 \t Acc 76.99 \t AccHead 77.76 \t AccTail 74.06\n",
      "Epoch: [034] \t Loss 0.3158 \t Acc 77.29 \t AccHead 78.69 \t AccTail 71.93\n",
      "Epoch: [035] \t Loss 0.3108 \t Acc 75.77 \t AccHead 77.52 \t AccTail 69.10\n",
      "Epoch: [036] \t Loss 0.2994 \t Acc 75.97 \t AccHead 77.70 \t AccTail 69.34\n",
      "Epoch: [037] \t Loss 0.2992 \t Acc 76.85 \t AccHead 78.44 \t AccTail 70.75\n",
      "Epoch: [038] \t Loss 0.3006 \t Acc 76.75 \t AccHead 78.81 \t AccTail 68.87\n",
      "Epoch: [039] \t Loss 0.2882 \t Acc 76.70 \t AccHead 78.07 \t AccTail 71.46\n",
      "Epoch: [040] \t Loss 0.2999 \t Acc 78.56 \t AccHead 81.84 \t AccTail 66.04\n",
      "Epoch: [041] \t Loss 0.2880 \t Acc 74.99 \t AccHead 77.39 \t AccTail 65.80\n",
      "Epoch: [042] \t Loss 0.2861 \t Acc 76.75 \t AccHead 78.01 \t AccTail 71.93\n",
      "Epoch: [043] \t Loss 0.2776 \t Acc 75.92 \t AccHead 77.76 \t AccTail 68.87\n",
      "Epoch: [044] \t Loss 0.2783 \t Acc 78.85 \t AccHead 81.59 \t AccTail 68.40\n",
      "Epoch: [045] \t Loss 0.2781 \t Acc 77.88 \t AccHead 79.74 \t AccTail 70.75\n",
      "Epoch: [046] \t Loss 0.2873 \t Acc 75.82 \t AccHead 77.27 \t AccTail 70.28\n",
      "Epoch: [047] \t Loss 0.2750 \t Acc 76.60 \t AccHead 78.38 \t AccTail 69.81\n",
      "Epoch: [048] \t Loss 0.2724 \t Acc 76.70 \t AccHead 78.88 \t AccTail 68.40\n",
      "Epoch: [049] \t Loss 0.2751 \t Acc 76.11 \t AccHead 76.96 \t AccTail 72.88\n",
      "Epoch: [050] \t Loss 0.2709 \t Acc 77.92 \t AccHead 79.06 \t AccTail 73.58\n",
      "Epoch: [051] \t Loss 0.2659 \t Acc 76.95 \t AccHead 79.31 \t AccTail 67.92\n",
      "Epoch: [052] \t Loss 0.2605 \t Acc 76.36 \t AccHead 77.64 \t AccTail 71.46\n",
      "Epoch: [053] \t Loss 0.2717 \t Acc 78.41 \t AccHead 81.84 \t AccTail 65.33\n",
      "Epoch: [054] \t Loss 0.2592 \t Acc 76.85 \t AccHead 78.63 \t AccTail 70.05\n",
      "Epoch: [055] \t Loss 0.2684 \t Acc 79.05 \t AccHead 80.85 \t AccTail 72.17\n",
      "Epoch: [056] \t Loss 0.2583 \t Acc 78.02 \t AccHead 80.17 \t AccTail 69.81\n",
      "Epoch: [057] \t Loss 0.2582 \t Acc 78.71 \t AccHead 80.67 \t AccTail 71.23\n",
      "Epoch: [058] \t Loss 0.2481 \t Acc 78.95 \t AccHead 81.16 \t AccTail 70.52\n",
      "Epoch: [059] \t Loss 0.2532 \t Acc 78.46 \t AccHead 81.66 \t AccTail 66.27\n",
      "Epoch: [060] \t Loss 0.2593 \t Acc 78.71 \t AccHead 81.59 \t AccTail 67.69\n",
      "Epoch: [061] \t Loss 0.2573 \t Acc 79.00 \t AccHead 82.21 \t AccTail 66.75\n",
      "Epoch: [062] \t Loss 0.2410 \t Acc 73.96 \t AccHead 74.61 \t AccTail 71.46\n",
      "Epoch: [063] \t Loss 0.2543 \t Acc 75.53 \t AccHead 78.26 \t AccTail 65.09\n",
      "Epoch: [064] \t Loss 0.2531 \t Acc 77.97 \t AccHead 79.25 \t AccTail 73.11\n",
      "Epoch: [065] \t Loss 0.2428 \t Acc 78.07 \t AccHead 81.96 \t AccTail 63.21\n",
      "Epoch: [066] \t Loss 0.2430 \t Acc 77.44 \t AccHead 78.69 \t AccTail 72.64\n",
      "Epoch: [067] \t Loss 0.2483 \t Acc 76.51 \t AccHead 78.44 \t AccTail 69.10\n",
      "Epoch: [068] \t Loss 0.2393 \t Acc 78.07 \t AccHead 79.99 \t AccTail 70.75\n",
      "Epoch: [069] \t Loss 0.2417 \t Acc 77.68 \t AccHead 80.17 \t AccTail 68.16\n",
      "Epoch: [070] \t Loss 0.2465 \t Acc 78.81 \t AccHead 81.90 \t AccTail 66.98\n",
      "Epoch: [071] \t Loss 0.2360 \t Acc 78.37 \t AccHead 80.91 \t AccTail 68.63\n",
      "Epoch: [072] \t Loss 0.2408 \t Acc 75.92 \t AccHead 76.47 \t AccTail 73.82\n",
      "Epoch: [073] \t Loss 0.2448 \t Acc 79.25 \t AccHead 81.22 \t AccTail 71.70\n",
      "Epoch: [074] \t Loss 0.2442 \t Acc 76.80 \t AccHead 78.69 \t AccTail 69.58\n",
      "Epoch: [075] \t Loss 0.2422 \t Acc 77.19 \t AccHead 80.67 \t AccTail 63.92\n",
      "Epoch: [076] \t Loss 0.2417 \t Acc 75.53 \t AccHead 77.89 \t AccTail 66.51\n",
      "Epoch: [077] \t Loss 0.2376 \t Acc 77.24 \t AccHead 80.67 \t AccTail 64.15\n",
      "Epoch: [078] \t Loss 0.2378 \t Acc 77.09 \t AccHead 78.63 \t AccTail 71.23\n",
      "Epoch: [079] \t Loss 0.2365 \t Acc 77.29 \t AccHead 80.91 \t AccTail 63.44\n",
      "Epoch: [080] \t Loss 0.2366 \t Acc 77.04 \t AccHead 79.31 \t AccTail 68.40\n",
      "Epoch: [081] \t Loss 0.2334 \t Acc 78.81 \t AccHead 80.91 \t AccTail 70.75\n",
      "Epoch: [082] \t Loss 0.2308 \t Acc 79.15 \t AccHead 80.91 \t AccTail 72.41\n",
      "Epoch: [083] \t Loss 0.2380 \t Acc 76.55 \t AccHead 77.95 \t AccTail 71.23\n",
      "Epoch: [084] \t Loss 0.2304 \t Acc 76.85 \t AccHead 80.23 \t AccTail 63.92\n",
      "Epoch: [085] \t Loss 0.2326 \t Acc 76.95 \t AccHead 77.95 \t AccTail 73.11\n",
      "Epoch: [086] \t Loss 0.2291 \t Acc 76.31 \t AccHead 79.00 \t AccTail 66.04\n",
      "Epoch: [087] \t Loss 0.2334 \t Acc 78.41 \t AccHead 79.86 \t AccTail 72.88\n",
      "Epoch: [088] \t Loss 0.2289 \t Acc 77.97 \t AccHead 79.80 \t AccTail 70.99\n",
      "Epoch: [089] \t Loss 0.2321 \t Acc 80.23 \t AccHead 83.76 \t AccTail 66.75\n",
      "Epoch: [090] \t Loss 0.2311 \t Acc 74.69 \t AccHead 76.90 \t AccTail 66.27\n",
      "Epoch: [091] \t Loss 0.2302 \t Acc 78.81 \t AccHead 81.78 \t AccTail 67.45\n",
      "Epoch: [092] \t Loss 0.2273 \t Acc 78.37 \t AccHead 81.28 \t AccTail 67.22\n",
      "Epoch: [093] \t Loss 0.2278 \t Acc 78.27 \t AccHead 82.27 \t AccTail 62.97\n",
      "Epoch: [094] \t Loss 0.2275 \t Acc 76.41 \t AccHead 79.25 \t AccTail 65.57\n",
      "Epoch: [095] \t Loss 0.2257 \t Acc 79.00 \t AccHead 82.58 \t AccTail 65.33\n",
      "Epoch: [096] \t Loss 0.2214 \t Acc 77.24 \t AccHead 79.25 \t AccTail 69.58\n",
      "Epoch: [097] \t Loss 0.2310 \t Acc 79.59 \t AccHead 82.83 \t AccTail 67.22\n",
      "Epoch: [098] \t Loss 0.2251 \t Acc 76.60 \t AccHead 78.94 \t AccTail 67.69\n",
      "Epoch: [099] \t Loss 0.2255 \t Acc 76.85 \t AccHead 79.37 \t AccTail 67.22\n",
      "Epoch: [100] \t Loss 0.2252 \t Acc 77.34 \t AccHead 78.88 \t AccTail 71.46\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    loss_history = []\n",
    "    model.train()\n",
    "    for batch_index, data in enumerate(train_loader):\n",
    "        image, target = data\n",
    "        image, target = image.cuda(), target.cuda()\n",
    "\n",
    "        pred = model(image)\n",
    "        loss = criterion(pred, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "    topk_acc, head_acc, tail_acc = compute_accuracy(val_loader, model)\n",
    "    loss_mean = np.mean(loss_history)\n",
    "    scheduler.step()\n",
    "\n",
    "    print('Epoch: [{:03d}] \\t Loss {:.4f} \\t Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(epoch+1, loss_mean, topk_acc[0], head_acc[0], tail_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3e2457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epoch': epoch},\n",
    "    osp.join(SAVE_DIR, 'ep{:03d}.pth'.format(epoch+1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d41d15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 72.91 \t AccHead 74.94 \t AccTail 70.88\n"
     ]
    }
   ],
   "source": [
    "##  CIFAR 10 : exp : 0.1 \n",
    "SAVE_DIR = 'logs/Tendency/featuresampling/CIFAR10/exp-0.1' \n",
    "model = ResNet18(num_classes)\n",
    "model.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model'])\n",
    "model = model.cuda()\n",
    "topk_acc, head_acc, tail_acc = compute_accuracy(test_loader, model)\n",
    "\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc[0], head_acc[0], tail_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb2e8b4",
   "metadata": {},
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be92501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = 'logs/Tendency/baseline/CIFAR10/exp-0.1' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb461b67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: MAKING DATASET ITERABLE\n"
     ]
    }
   ],
   "source": [
    "print('STEP 2: MAKING DATASET ITERABLE')\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=None,\n",
    "    num_workers=4, drop_last=False)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    validation_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=4, drop_last=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=100, shuffle=False,\n",
    "    num_workers=4, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "78036252",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: INSTANTIATE MODEL CLASS\n",
      "STEP 5: INSTANTIATE LOSS CLASS\n"
     ]
    }
   ],
   "source": [
    "print('STEP 4: INSTANTIATE MODEL CLASS')\n",
    "\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "\n",
    "print('STEP 5: INSTANTIATE LOSS CLASS')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f063951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea710170-226d-4093-84e0-cacbee35ed81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [001] \t Loss 2.5109 \t Acc 41.65 \t AccHead 52.56 \t AccTail 0.00\n",
      "Epoch: [002] \t Loss 1.6600 \t Acc 45.03 \t AccHead 54.35 \t AccTail 9.43\n",
      "Epoch: [003] \t Loss 1.5383 \t Acc 47.87 \t AccHead 54.35 \t AccTail 23.11\n",
      "Epoch: [004] \t Loss 1.4318 \t Acc 50.27 \t AccHead 57.38 \t AccTail 23.11\n",
      "Epoch: [005] \t Loss 1.3672 \t Acc 53.35 \t AccHead 60.90 \t AccTail 24.53\n",
      "Epoch: [006] \t Loss 1.3262 \t Acc 54.14 \t AccHead 61.58 \t AccTail 25.71\n",
      "Epoch: [007] \t Loss 1.2877 \t Acc 56.68 \t AccHead 63.68 \t AccTail 29.95\n",
      "Epoch: [008] \t Loss 1.2077 \t Acc 55.21 \t AccHead 61.40 \t AccTail 31.60\n",
      "Epoch: [009] \t Loss 1.1439 \t Acc 59.91 \t AccHead 67.08 \t AccTail 32.55\n",
      "Epoch: [010] \t Loss 1.0994 \t Acc 62.41 \t AccHead 68.99 \t AccTail 37.26\n",
      "Epoch: [011] \t Loss 1.0377 \t Acc 62.26 \t AccHead 67.70 \t AccTail 41.51\n",
      "Epoch: [012] \t Loss 0.9979 \t Acc 64.12 \t AccHead 68.93 \t AccTail 45.75\n",
      "Epoch: [013] \t Loss 0.9654 \t Acc 65.10 \t AccHead 71.90 \t AccTail 39.15\n",
      "Epoch: [014] \t Loss 0.9290 \t Acc 64.86 \t AccHead 69.92 \t AccTail 45.52\n",
      "Epoch: [015] \t Loss 0.9036 \t Acc 67.30 \t AccHead 73.19 \t AccTail 44.81\n",
      "Epoch: [016] \t Loss 0.8746 \t Acc 70.44 \t AccHead 74.06 \t AccTail 56.60\n",
      "Epoch: [017] \t Loss 0.8454 \t Acc 66.23 \t AccHead 69.86 \t AccTail 52.36\n",
      "Epoch: [018] \t Loss 0.8237 \t Acc 69.95 \t AccHead 74.18 \t AccTail 53.77\n",
      "Epoch: [019] \t Loss 0.7976 \t Acc 70.68 \t AccHead 74.37 \t AccTail 56.60\n",
      "Epoch: [020] \t Loss 0.7813 \t Acc 70.29 \t AccHead 73.13 \t AccTail 59.43\n",
      "Epoch: [021] \t Loss 0.7543 \t Acc 71.95 \t AccHead 76.34 \t AccTail 55.19\n",
      "Epoch: [022] \t Loss 0.7490 \t Acc 71.02 \t AccHead 74.61 \t AccTail 57.31\n",
      "Epoch: [023] \t Loss 0.7217 \t Acc 73.23 \t AccHead 76.53 \t AccTail 60.61\n",
      "Epoch: [024] \t Loss 0.7013 \t Acc 72.10 \t AccHead 74.43 \t AccTail 63.21\n",
      "Epoch: [025] \t Loss 0.6887 \t Acc 73.76 \t AccHead 76.53 \t AccTail 63.21\n",
      "Epoch: [026] \t Loss 0.6910 \t Acc 74.50 \t AccHead 77.76 \t AccTail 62.03\n",
      "Epoch: [027] \t Loss 0.6695 \t Acc 73.47 \t AccHead 76.03 \t AccTail 63.68\n",
      "Epoch: [028] \t Loss 0.6459 \t Acc 72.20 \t AccHead 75.17 \t AccTail 60.85\n",
      "Epoch: [029] \t Loss 0.6443 \t Acc 71.86 \t AccHead 76.78 \t AccTail 53.07\n",
      "Epoch: [030] \t Loss 0.6180 \t Acc 72.44 \t AccHead 74.86 \t AccTail 63.21\n",
      "Epoch: [031] \t Loss 0.6078 \t Acc 74.40 \t AccHead 78.57 \t AccTail 58.49\n",
      "Epoch: [032] \t Loss 0.5953 \t Acc 73.37 \t AccHead 76.41 \t AccTail 61.79\n",
      "Epoch: [033] \t Loss 0.5915 \t Acc 72.54 \t AccHead 76.34 \t AccTail 58.02\n",
      "Epoch: [034] \t Loss 0.5777 \t Acc 75.04 \t AccHead 79.31 \t AccTail 58.73\n",
      "Epoch: [035] \t Loss 0.5722 \t Acc 73.37 \t AccHead 76.90 \t AccTail 59.91\n",
      "Epoch: [036] \t Loss 0.5632 \t Acc 72.64 \t AccHead 75.11 \t AccTail 63.21\n",
      "Epoch: [037] \t Loss 0.5551 \t Acc 75.43 \t AccHead 78.07 \t AccTail 65.33\n",
      "Epoch: [038] \t Loss 0.5562 \t Acc 74.40 \t AccHead 78.26 \t AccTail 59.67\n",
      "Epoch: [039] \t Loss 0.5407 \t Acc 73.03 \t AccHead 77.46 \t AccTail 56.13\n",
      "Epoch: [040] \t Loss 0.5311 \t Acc 73.96 \t AccHead 77.58 \t AccTail 60.14\n",
      "Epoch: [041] \t Loss 0.5211 \t Acc 73.96 \t AccHead 77.27 \t AccTail 61.32\n",
      "Epoch: [042] \t Loss 0.5207 \t Acc 73.47 \t AccHead 77.52 \t AccTail 58.02\n",
      "Epoch: [043] \t Loss 0.5026 \t Acc 75.38 \t AccHead 79.62 \t AccTail 59.20\n",
      "Epoch: [044] \t Loss 0.5041 \t Acc 72.30 \t AccHead 76.03 \t AccTail 58.02\n",
      "Epoch: [045] \t Loss 0.4917 \t Acc 76.41 \t AccHead 80.17 \t AccTail 62.03\n",
      "Epoch: [046] \t Loss 0.4772 \t Acc 76.31 \t AccHead 80.98 \t AccTail 58.49\n",
      "Epoch: [047] \t Loss 0.4891 \t Acc 76.55 \t AccHead 80.11 \t AccTail 62.97\n",
      "Epoch: [048] \t Loss 0.4724 \t Acc 74.06 \t AccHead 76.28 \t AccTail 65.57\n",
      "Epoch: [049] \t Loss 0.4684 \t Acc 74.01 \t AccHead 77.70 \t AccTail 59.91\n",
      "Epoch: [050] \t Loss 0.4573 \t Acc 76.26 \t AccHead 81.35 \t AccTail 56.84\n",
      "Epoch: [051] \t Loss 0.4599 \t Acc 74.99 \t AccHead 77.76 \t AccTail 64.39\n",
      "Epoch: [052] \t Loss 0.4586 \t Acc 74.99 \t AccHead 78.51 \t AccTail 61.56\n",
      "Epoch: [053] \t Loss 0.4639 \t Acc 75.97 \t AccHead 78.94 \t AccTail 64.62\n",
      "Epoch: [054] \t Loss 0.4453 \t Acc 73.72 \t AccHead 75.73 \t AccTail 66.04\n",
      "Epoch: [055] \t Loss 0.4462 \t Acc 74.20 \t AccHead 76.71 \t AccTail 64.62\n",
      "Epoch: [056] \t Loss 0.4212 \t Acc 76.65 \t AccHead 80.48 \t AccTail 62.03\n",
      "Epoch: [057] \t Loss 0.4269 \t Acc 77.68 \t AccHead 81.78 \t AccTail 62.03\n",
      "Epoch: [058] \t Loss 0.4284 \t Acc 76.55 \t AccHead 81.22 \t AccTail 58.73\n",
      "Epoch: [059] \t Loss 0.4231 \t Acc 75.09 \t AccHead 78.69 \t AccTail 61.32\n",
      "Epoch: [060] \t Loss 0.4106 \t Acc 74.35 \t AccHead 78.69 \t AccTail 57.78\n",
      "Epoch: [061] \t Loss 0.4104 \t Acc 76.70 \t AccHead 80.23 \t AccTail 63.21\n",
      "Epoch: [062] \t Loss 0.4104 \t Acc 75.09 \t AccHead 78.07 \t AccTail 63.68\n",
      "Epoch: [063] \t Loss 0.4115 \t Acc 76.41 \t AccHead 80.36 \t AccTail 61.32\n",
      "Epoch: [064] \t Loss 0.4093 \t Acc 75.53 \t AccHead 78.20 \t AccTail 65.33\n",
      "Epoch: [065] \t Loss 0.3942 \t Acc 73.86 \t AccHead 77.52 \t AccTail 59.91\n",
      "Epoch: [066] \t Loss 0.3928 \t Acc 74.45 \t AccHead 77.33 \t AccTail 63.44\n",
      "Epoch: [067] \t Loss 0.3934 \t Acc 77.63 \t AccHead 80.67 \t AccTail 66.04\n",
      "Epoch: [068] \t Loss 0.3961 \t Acc 75.77 \t AccHead 80.98 \t AccTail 55.90\n",
      "Epoch: [069] \t Loss 0.3792 \t Acc 76.85 \t AccHead 81.28 \t AccTail 59.91\n",
      "Epoch: [070] \t Loss 0.3776 \t Acc 77.92 \t AccHead 80.98 \t AccTail 66.27\n",
      "Epoch: [071] \t Loss 0.3744 \t Acc 77.48 \t AccHead 81.90 \t AccTail 60.61\n",
      "Epoch: [072] \t Loss 0.3593 \t Acc 75.48 \t AccHead 80.30 \t AccTail 57.08\n",
      "Epoch: [073] \t Loss 0.3671 \t Acc 75.43 \t AccHead 78.57 \t AccTail 63.44\n",
      "Epoch: [074] \t Loss 0.3696 \t Acc 76.70 \t AccHead 80.61 \t AccTail 61.79\n",
      "Epoch: [075] \t Loss 0.3644 \t Acc 76.85 \t AccHead 80.98 \t AccTail 61.08\n",
      "Epoch: [076] \t Loss 0.3510 \t Acc 76.65 \t AccHead 78.57 \t AccTail 69.34\n",
      "Epoch: [077] \t Loss 0.3451 \t Acc 77.83 \t AccHead 80.98 \t AccTail 65.80\n",
      "Epoch: [078] \t Loss 0.3591 \t Acc 75.77 \t AccHead 78.88 \t AccTail 63.92\n",
      "Epoch: [079] \t Loss 0.3585 \t Acc 78.32 \t AccHead 82.21 \t AccTail 63.44\n",
      "Epoch: [080] \t Loss 0.3485 \t Acc 76.36 \t AccHead 80.79 \t AccTail 59.43\n",
      "Epoch: [081] \t Loss 0.3470 \t Acc 77.09 \t AccHead 80.42 \t AccTail 64.39\n",
      "Epoch: [082] \t Loss 0.3327 \t Acc 75.33 \t AccHead 79.18 \t AccTail 60.61\n",
      "Epoch: [083] \t Loss 0.3360 \t Acc 76.90 \t AccHead 80.91 \t AccTail 61.56\n",
      "Epoch: [084] \t Loss 0.3316 \t Acc 76.16 \t AccHead 79.25 \t AccTail 64.39\n",
      "Epoch: [085] \t Loss 0.3379 \t Acc 75.09 \t AccHead 79.80 \t AccTail 57.08\n",
      "Epoch: [086] \t Loss 0.3452 \t Acc 77.68 \t AccHead 80.98 \t AccTail 65.09\n",
      "Epoch: [087] \t Loss 0.3231 \t Acc 76.26 \t AccHead 80.11 \t AccTail 61.56\n",
      "Epoch: [088] \t Loss 0.3351 \t Acc 75.09 \t AccHead 77.52 \t AccTail 65.80\n",
      "Epoch: [089] \t Loss 0.3138 \t Acc 77.24 \t AccHead 79.99 \t AccTail 66.75\n",
      "Epoch: [090] \t Loss 0.3271 \t Acc 75.48 \t AccHead 78.81 \t AccTail 62.74\n",
      "Epoch: [091] \t Loss 0.3265 \t Acc 75.72 \t AccHead 80.05 \t AccTail 59.20\n",
      "Epoch: [092] \t Loss 0.3250 \t Acc 77.48 \t AccHead 79.93 \t AccTail 68.16\n",
      "Epoch: [093] \t Loss 0.3209 \t Acc 74.94 \t AccHead 78.38 \t AccTail 61.79\n",
      "Epoch: [094] \t Loss 0.3066 \t Acc 77.34 \t AccHead 80.91 \t AccTail 63.68\n",
      "Epoch: [095] \t Loss 0.3204 \t Acc 79.20 \t AccHead 83.14 \t AccTail 64.15\n",
      "Epoch: [096] \t Loss 0.3157 \t Acc 75.23 \t AccHead 76.71 \t AccTail 69.58\n",
      "Epoch: [097] \t Loss 0.3181 \t Acc 75.67 \t AccHead 79.80 \t AccTail 59.91\n",
      "Epoch: [098] \t Loss 0.3013 \t Acc 76.11 \t AccHead 81.04 \t AccTail 57.31\n",
      "Epoch: [099] \t Loss 0.3088 \t Acc 76.65 \t AccHead 80.61 \t AccTail 61.56\n",
      "Epoch: [100] \t Loss 0.3076 \t Acc 78.71 \t AccHead 81.66 \t AccTail 67.45\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    loss_history = []\n",
    "    model.train()\n",
    "    for batch_index, data in enumerate(train_loader):\n",
    "        image, target = data\n",
    "        image, target = image.cuda(), target.cuda()\n",
    "\n",
    "        pred = model(image)\n",
    "        loss = criterion(pred, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "    topk_acc, head_acc, tail_acc = compute_accuracy(val_loader, model)\n",
    "    loss_mean = np.mean(loss_history)\n",
    "    scheduler.step()\n",
    "\n",
    "    print('Epoch: [{:03d}] \\t Loss {:.4f} \\t Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(epoch+1, loss_mean, topk_acc[0], head_acc[0], tail_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ebfa934f-3869-4519-86e1-abd9173b62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "87690c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epoch': epoch},\n",
    "    osp.join(SAVE_DIR, 'ep{:03d}.pth'.format(epoch+1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1022100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 67.52 \t AccHead 73.42 \t AccTail 61.62\n"
     ]
    }
   ],
   "source": [
    "##  CIFAR 10 : exp : 0.1 \n",
    "SAVE_DIR = 'logs/Tendency/baseline/CIFAR10/exp-0.1' \n",
    "model = ResNet18(num_classes)\n",
    "model.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model'])\n",
    "model = model.cuda()\n",
    "topk_acc, head_acc, tail_acc = compute_accuracy(test_loader, model)\n",
    "\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc[0], head_acc[0], tail_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f864588",
   "metadata": {},
   "source": [
    "여기서부터는 LWS 기법이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be92501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = 'logs/Tendency/LWS/CIFAR10/exp-0.1' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e0de42ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: CREATE MODEL CLASS (VGG16)\n"
     ]
    }
   ],
   "source": [
    "print('STEP 3: CREATE MODEL CLASS (VGG16)')\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.backbone = resnet18()\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "        self.scales = Parameter(torch.ones(num_classes))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone(x)\n",
    "        x = F.adaptive_max_pool2d(x, 1)\n",
    "        x = x.view(batch_size, -1)\n",
    "        pred = self.classifier(x)\n",
    "        # LWS 구현\n",
    "        pred *= self.scales\n",
    "#         print(pred.size()) # torch.Size([128, 10])\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "afdb726e-f03a-4032-904f-deb21613086a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: INSTANTIATE MODEL CLASS\n",
      "STEP 5: INSTANTIATE LOSS CLASS\n"
     ]
    }
   ],
   "source": [
    "print('STEP 4: INSTANTIATE MODEL CLASS')\n",
    "\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "\n",
    "print('STEP 5: INSTANTIATE LOSS CLASS')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a19d7af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea710170-226d-4093-84e0-cacbee35ed81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [001] \t Loss 3.0158 \t Acc 32.11 \t AccHead 40.52 \t AccTail 0.00\n",
      "Epoch: [002] \t Loss 1.7633 \t Acc 41.80 \t AccHead 52.75 \t AccTail 0.00\n",
      "Epoch: [003] \t Loss 1.6069 \t Acc 44.20 \t AccHead 55.34 \t AccTail 1.65\n",
      "Epoch: [004] \t Loss 1.5288 \t Acc 46.06 \t AccHead 55.53 \t AccTail 9.91\n",
      "Epoch: [005] \t Loss 1.4778 \t Acc 48.31 \t AccHead 56.08 \t AccTail 18.63\n",
      "Epoch: [006] \t Loss 1.4209 \t Acc 50.91 \t AccHead 58.55 \t AccTail 21.70\n",
      "Epoch: [007] \t Loss 1.3588 \t Acc 52.86 \t AccHead 60.84 \t AccTail 22.41\n",
      "Epoch: [008] \t Loss 1.3142 \t Acc 52.86 \t AccHead 59.48 \t AccTail 27.59\n",
      "Epoch: [009] \t Loss 1.2715 \t Acc 54.38 \t AccHead 61.15 \t AccTail 28.54\n",
      "Epoch: [010] \t Loss 1.2255 \t Acc 56.93 \t AccHead 63.50 \t AccTail 31.84\n",
      "Epoch: [011] \t Loss 1.1729 \t Acc 57.17 \t AccHead 63.93 \t AccTail 31.37\n",
      "Epoch: [012] \t Loss 1.1222 \t Acc 60.16 \t AccHead 67.33 \t AccTail 32.78\n",
      "Epoch: [013] \t Loss 1.0824 \t Acc 62.80 \t AccHead 68.44 \t AccTail 41.27\n",
      "Epoch: [014] \t Loss 1.0331 \t Acc 61.14 \t AccHead 67.33 \t AccTail 37.50\n",
      "Epoch: [015] \t Loss 1.0025 \t Acc 63.68 \t AccHead 69.24 \t AccTail 42.45\n",
      "Epoch: [016] \t Loss 0.9607 \t Acc 63.14 \t AccHead 69.18 \t AccTail 40.09\n",
      "Epoch: [017] \t Loss 0.9359 \t Acc 64.81 \t AccHead 67.63 \t AccTail 54.01\n",
      "Epoch: [018] \t Loss 0.9111 \t Acc 64.27 \t AccHead 68.38 \t AccTail 48.58\n",
      "Epoch: [019] \t Loss 0.8853 \t Acc 66.23 \t AccHead 70.11 \t AccTail 51.42\n",
      "Epoch: [020] \t Loss 0.8673 \t Acc 66.86 \t AccHead 70.04 \t AccTail 54.72\n",
      "Epoch: [021] \t Loss 0.8424 \t Acc 68.28 \t AccHead 70.85 \t AccTail 58.49\n",
      "Epoch: [022] \t Loss 0.8279 \t Acc 66.23 \t AccHead 69.18 \t AccTail 54.95\n",
      "Epoch: [023] \t Loss 0.8031 \t Acc 69.70 \t AccHead 73.13 \t AccTail 56.60\n",
      "Epoch: [024] \t Loss 0.7887 \t Acc 69.02 \t AccHead 74.00 \t AccTail 50.00\n",
      "Epoch: [025] \t Loss 0.7646 \t Acc 70.78 \t AccHead 74.31 \t AccTail 57.31\n",
      "Epoch: [026] \t Loss 0.7455 \t Acc 70.29 \t AccHead 74.55 \t AccTail 54.01\n",
      "Epoch: [027] \t Loss 0.7332 \t Acc 70.78 \t AccHead 73.87 \t AccTail 58.96\n",
      "Epoch: [028] \t Loss 0.7295 \t Acc 71.81 \t AccHead 76.10 \t AccTail 55.42\n",
      "Epoch: [029] \t Loss 0.7069 \t Acc 70.73 \t AccHead 74.80 \t AccTail 55.19\n",
      "Epoch: [030] \t Loss 0.7000 \t Acc 70.68 \t AccHead 74.55 \t AccTail 55.90\n",
      "Epoch: [031] \t Loss 0.6850 \t Acc 70.68 \t AccHead 74.68 \t AccTail 55.42\n",
      "Epoch: [032] \t Loss 0.6757 \t Acc 72.64 \t AccHead 76.65 \t AccTail 57.31\n",
      "Epoch: [033] \t Loss 0.6610 \t Acc 70.97 \t AccHead 75.05 \t AccTail 55.42\n",
      "Epoch: [034] \t Loss 0.6557 \t Acc 72.59 \t AccHead 75.85 \t AccTail 60.14\n",
      "Epoch: [035] \t Loss 0.6363 \t Acc 72.54 \t AccHead 76.28 \t AccTail 58.25\n",
      "Epoch: [036] \t Loss 0.6432 \t Acc 71.76 \t AccHead 74.37 \t AccTail 61.79\n",
      "Epoch: [037] \t Loss 0.6219 \t Acc 73.13 \t AccHead 75.17 \t AccTail 65.33\n",
      "Epoch: [038] \t Loss 0.6171 \t Acc 71.37 \t AccHead 72.70 \t AccTail 66.27\n",
      "Epoch: [039] \t Loss 0.6020 \t Acc 71.90 \t AccHead 75.54 \t AccTail 58.02\n",
      "Epoch: [040] \t Loss 0.5877 \t Acc 75.53 \t AccHead 79.80 \t AccTail 59.20\n",
      "Epoch: [041] \t Loss 0.5798 \t Acc 73.62 \t AccHead 78.20 \t AccTail 56.13\n",
      "Epoch: [042] \t Loss 0.5791 \t Acc 72.44 \t AccHead 74.06 \t AccTail 66.27\n",
      "Epoch: [043] \t Loss 0.5636 \t Acc 74.11 \t AccHead 78.94 \t AccTail 55.66\n",
      "Epoch: [044] \t Loss 0.5634 \t Acc 74.60 \t AccHead 79.86 \t AccTail 54.48\n",
      "Epoch: [045] \t Loss 0.5489 \t Acc 74.40 \t AccHead 77.95 \t AccTail 60.85\n",
      "Epoch: [046] \t Loss 0.5513 \t Acc 71.27 \t AccHead 73.87 \t AccTail 61.32\n",
      "Epoch: [047] \t Loss 0.5447 \t Acc 72.83 \t AccHead 75.79 \t AccTail 61.56\n",
      "Epoch: [048] \t Loss 0.5303 \t Acc 74.60 \t AccHead 78.32 \t AccTail 60.38\n",
      "Epoch: [049] \t Loss 0.5232 \t Acc 75.82 \t AccHead 81.35 \t AccTail 54.72\n",
      "Epoch: [050] \t Loss 0.5167 \t Acc 74.01 \t AccHead 77.46 \t AccTail 60.85\n",
      "Epoch: [051] \t Loss 0.5163 \t Acc 75.04 \t AccHead 79.18 \t AccTail 59.20\n",
      "Epoch: [052] \t Loss 0.5077 \t Acc 76.31 \t AccHead 80.79 \t AccTail 59.20\n",
      "Epoch: [053] \t Loss 0.4933 \t Acc 74.84 \t AccHead 77.46 \t AccTail 64.86\n",
      "Epoch: [054] \t Loss 0.4940 \t Acc 76.26 \t AccHead 79.56 \t AccTail 63.68\n",
      "Epoch: [055] \t Loss 0.4858 \t Acc 75.09 \t AccHead 78.51 \t AccTail 62.03\n",
      "Epoch: [056] \t Loss 0.4814 \t Acc 74.35 \t AccHead 78.01 \t AccTail 60.38\n",
      "Epoch: [057] \t Loss 0.4736 \t Acc 74.55 \t AccHead 78.20 \t AccTail 60.61\n",
      "Epoch: [058] \t Loss 0.4787 \t Acc 74.30 \t AccHead 77.02 \t AccTail 63.92\n",
      "Epoch: [059] \t Loss 0.4694 \t Acc 75.23 \t AccHead 77.52 \t AccTail 66.51\n",
      "Epoch: [060] \t Loss 0.4636 \t Acc 74.45 \t AccHead 76.53 \t AccTail 66.51\n",
      "Epoch: [061] \t Loss 0.4434 \t Acc 75.28 \t AccHead 78.13 \t AccTail 64.39\n",
      "Epoch: [062] \t Loss 0.4546 \t Acc 76.21 \t AccHead 79.93 \t AccTail 62.03\n",
      "Epoch: [063] \t Loss 0.4505 \t Acc 74.06 \t AccHead 76.10 \t AccTail 66.27\n",
      "Epoch: [064] \t Loss 0.4480 \t Acc 75.92 \t AccHead 79.37 \t AccTail 62.74\n",
      "Epoch: [065] \t Loss 0.4390 \t Acc 76.21 \t AccHead 78.44 \t AccTail 67.69\n",
      "Epoch: [066] \t Loss 0.4369 \t Acc 75.62 \t AccHead 78.94 \t AccTail 62.97\n",
      "Epoch: [067] \t Loss 0.4368 \t Acc 75.77 \t AccHead 78.88 \t AccTail 63.92\n",
      "Epoch: [068] \t Loss 0.4258 \t Acc 74.65 \t AccHead 76.16 \t AccTail 68.87\n",
      "Epoch: [069] \t Loss 0.4241 \t Acc 74.45 \t AccHead 77.70 \t AccTail 62.03\n",
      "Epoch: [070] \t Loss 0.4200 \t Acc 74.89 \t AccHead 77.64 \t AccTail 64.39\n",
      "Epoch: [071] \t Loss 0.4124 \t Acc 74.35 \t AccHead 78.32 \t AccTail 59.20\n",
      "Epoch: [072] \t Loss 0.4105 \t Acc 76.26 \t AccHead 79.49 \t AccTail 63.92\n",
      "Epoch: [073] \t Loss 0.4069 \t Acc 75.33 \t AccHead 79.06 \t AccTail 61.08\n",
      "Epoch: [074] \t Loss 0.4095 \t Acc 73.32 \t AccHead 76.78 \t AccTail 60.14\n",
      "Epoch: [075] \t Loss 0.4158 \t Acc 75.13 \t AccHead 78.57 \t AccTail 62.03\n",
      "Epoch: [076] \t Loss 0.3960 \t Acc 75.97 \t AccHead 79.06 \t AccTail 64.15\n",
      "Epoch: [077] \t Loss 0.3989 \t Acc 74.50 \t AccHead 78.94 \t AccTail 57.55\n",
      "Epoch: [078] \t Loss 0.3887 \t Acc 76.11 \t AccHead 80.05 \t AccTail 61.08\n",
      "Epoch: [079] \t Loss 0.3872 \t Acc 74.55 \t AccHead 79.49 \t AccTail 55.66\n",
      "Epoch: [080] \t Loss 0.3861 \t Acc 76.46 \t AccHead 80.73 \t AccTail 60.14\n",
      "Epoch: [081] \t Loss 0.3738 \t Acc 75.38 \t AccHead 79.31 \t AccTail 60.38\n",
      "Epoch: [082] \t Loss 0.3726 \t Acc 74.35 \t AccHead 79.18 \t AccTail 55.90\n",
      "Epoch: [083] \t Loss 0.3698 \t Acc 76.70 \t AccHead 80.05 \t AccTail 63.92\n",
      "Epoch: [084] \t Loss 0.3728 \t Acc 74.94 \t AccHead 80.11 \t AccTail 55.19\n",
      "Epoch: [085] \t Loss 0.3728 \t Acc 75.48 \t AccHead 79.31 \t AccTail 60.85\n",
      "Epoch: [086] \t Loss 0.3667 \t Acc 76.70 \t AccHead 81.53 \t AccTail 58.25\n",
      "Epoch: [087] \t Loss 0.3596 \t Acc 76.70 \t AccHead 80.85 \t AccTail 60.85\n",
      "Epoch: [088] \t Loss 0.3686 \t Acc 74.84 \t AccHead 80.23 \t AccTail 54.25\n",
      "Epoch: [089] \t Loss 0.3704 \t Acc 74.65 \t AccHead 76.53 \t AccTail 67.45\n",
      "Epoch: [090] \t Loss 0.3698 \t Acc 77.14 \t AccHead 81.10 \t AccTail 62.03\n",
      "Epoch: [091] \t Loss 0.3609 \t Acc 73.13 \t AccHead 75.60 \t AccTail 63.68\n",
      "Epoch: [092] \t Loss 0.3567 \t Acc 76.75 \t AccHead 82.15 \t AccTail 56.13\n",
      "Epoch: [093] \t Loss 0.3498 \t Acc 75.72 \t AccHead 79.18 \t AccTail 62.50\n",
      "Epoch: [094] \t Loss 0.3539 \t Acc 77.24 \t AccHead 81.53 \t AccTail 60.85\n",
      "Epoch: [095] \t Loss 0.3531 \t Acc 76.51 \t AccHead 81.66 \t AccTail 56.84\n",
      "Epoch: [096] \t Loss 0.3449 \t Acc 75.77 \t AccHead 79.68 \t AccTail 60.85\n",
      "Epoch: [097] \t Loss 0.3524 \t Acc 77.73 \t AccHead 80.79 \t AccTail 66.04\n",
      "Epoch: [098] \t Loss 0.3375 \t Acc 76.31 \t AccHead 81.59 \t AccTail 56.13\n",
      "Epoch: [099] \t Loss 0.3388 \t Acc 76.16 \t AccHead 80.30 \t AccTail 60.38\n",
      "Epoch: [100] \t Loss 0.3397 \t Acc 75.23 \t AccHead 78.44 \t AccTail 62.97\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    loss_history = []\n",
    "    model.train()\n",
    "    for batch_index, data in enumerate(train_loader):\n",
    "        image, target = data\n",
    "        image, target = image.cuda(), target.cuda()\n",
    "\n",
    "        pred = model(image)\n",
    "        loss = criterion(pred, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "    topk_acc, head_acc, tail_acc = compute_accuracy(val_loader, model)\n",
    "    loss_mean = np.mean(loss_history)\n",
    "    scheduler.step()\n",
    "\n",
    "    print('Epoch: [{:03d}] \\t Loss {:.4f} \\t Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(epoch+1, loss_mean, topk_acc[0], head_acc[0], tail_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ebfa934f-3869-4519-86e1-abd9173b62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "615dcc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epoch': epoch},\n",
    "    osp.join(SAVE_DIR, 'ep{:03d}.pth'.format(epoch+1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e1d5cb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 67.40 \t AccHead 72.84 \t AccTail 61.96\n"
     ]
    }
   ],
   "source": [
    "##  CIFAR 10 : exp : 0.1 \n",
    "SAVE_DIR = 'logs/Tendency/LWS/CIFAR10/exp-0.1' \n",
    "model.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model'])\n",
    "model = model.cuda()\n",
    "topk_acc, head_acc, tail_acc = compute_accuracy(test_loader, model)\n",
    "\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc[0], head_acc[0], tail_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d145df",
   "metadata": {},
   "source": [
    "여기서 부터는 mymodel이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be92501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = 'logs/Tendency/mymodel/CIFAR10/exp-0.1' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c0f9a82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: INSTANTIATE MODEL CLASS\n",
      "STEP 5: INSTANTIATE LOSS CLASS\n"
     ]
    }
   ],
   "source": [
    "print('STEP 4: INSTANTIATE MODEL CLASS')\n",
    "\n",
    "model_head = ResNet18(num_classes)\n",
    "model_tail = ResNet18(num_classes)\n",
    "model_head = model_head.cuda()\n",
    "model_tail = model_tail.cuda()\n",
    "\n",
    "print('STEP 5: INSTANTIATE LOSS CLASS')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_head = torch.optim.SGD(model_head.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "optimizer_tail = torch.optim.SGD(model_tail.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler_head = torch.optim.lr_scheduler.StepLR(optimizer_head, step_size=150, gamma=0.1)\n",
    "scheduler_tail = torch.optim.lr_scheduler.StepLR(optimizer_tail, step_size=150, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3fe3ca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8eeff0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 7: TRAIN THE MODEL\n",
      "Epoch: [001] \t Loss 4.3781 \t Acc 39.31 \t AccHead 44.66 \t AccTail 33.96\n",
      "Epoch: [002] \t Loss 2.6075 \t Acc 50.38 \t AccHead 56.89 \t AccTail 43.87\n",
      "Epoch: [003] \t Loss 2.3109 \t Acc 54.40 \t AccHead 61.40 \t AccTail 47.41\n",
      "Epoch: [004] \t Loss 2.1666 \t Acc 58.87 \t AccHead 64.67 \t AccTail 53.07\n",
      "Epoch: [005] \t Loss 2.0282 \t Acc 62.12 \t AccHead 66.46 \t AccTail 57.78\n",
      "Epoch: [006] \t Loss 1.8902 \t Acc 65.48 \t AccHead 67.51 \t AccTail 63.44\n",
      "Epoch: [007] \t Loss 1.8220 \t Acc 65.88 \t AccHead 67.14 \t AccTail 64.62\n",
      "Epoch: [008] \t Loss 1.7210 \t Acc 66.83 \t AccHead 69.98 \t AccTail 63.68\n",
      "Epoch: [009] \t Loss 1.6418 \t Acc 68.98 \t AccHead 71.22 \t AccTail 66.75\n",
      "Epoch: [010] \t Loss 1.5464 \t Acc 71.63 \t AccHead 73.44 \t AccTail 69.81\n",
      "Epoch: [011] \t Loss 1.5074 \t Acc 73.55 \t AccHead 74.92 \t AccTail 72.17\n",
      "Epoch: [012] \t Loss 1.4389 \t Acc 70.68 \t AccHead 74.61 \t AccTail 66.75\n",
      "Epoch: [013] \t Loss 1.3983 \t Acc 73.66 \t AccHead 77.52 \t AccTail 69.81\n",
      "Epoch: [014] \t Loss 1.3497 \t Acc 74.07 \t AccHead 75.73 \t AccTail 72.41\n",
      "Epoch: [015] \t Loss 1.2732 \t Acc 74.21 \t AccHead 76.96 \t AccTail 71.46\n",
      "Epoch: [016] \t Loss 1.2462 \t Acc 74.44 \t AccHead 75.54 \t AccTail 73.35\n",
      "Epoch: [017] \t Loss 1.2090 \t Acc 76.86 \t AccHead 79.43 \t AccTail 74.29\n",
      "Epoch: [018] \t Loss 1.1971 \t Acc 74.64 \t AccHead 76.16 \t AccTail 73.11\n",
      "Epoch: [019] \t Loss 1.1625 \t Acc 76.35 \t AccHead 80.05 \t AccTail 72.64\n",
      "Epoch: [020] \t Loss 1.1163 \t Acc 73.78 \t AccHead 78.69 \t AccTail 68.87\n",
      "Epoch: [021] \t Loss 1.1260 \t Acc 75.64 \t AccHead 80.30 \t AccTail 70.99\n",
      "Epoch: [022] \t Loss 1.0752 \t Acc 77.46 \t AccHead 79.93 \t AccTail 75.00\n",
      "Epoch: [023] \t Loss 1.0473 \t Acc 77.19 \t AccHead 81.04 \t AccTail 73.35\n",
      "Epoch: [024] \t Loss 1.0354 \t Acc 78.19 \t AccHead 80.91 \t AccTail 75.47\n",
      "Epoch: [025] \t Loss 0.9825 \t Acc 77.23 \t AccHead 81.35 \t AccTail 73.11\n",
      "Epoch: [026] \t Loss 1.0098 \t Acc 77.69 \t AccHead 82.03 \t AccTail 73.35\n",
      "Epoch: [027] \t Loss 0.9733 \t Acc 78.32 \t AccHead 78.57 \t AccTail 78.07\n",
      "Epoch: [028] \t Loss 0.9601 \t Acc 76.76 \t AccHead 76.41 \t AccTail 77.12\n",
      "Epoch: [029] \t Loss 0.9358 \t Acc 77.08 \t AccHead 80.11 \t AccTail 74.06\n",
      "Epoch: [030] \t Loss 0.9247 \t Acc 78.32 \t AccHead 81.16 \t AccTail 75.47\n",
      "Epoch: [031] \t Loss 0.9060 \t Acc 79.65 \t AccHead 81.47 \t AccTail 77.83\n",
      "Epoch: [032] \t Loss 0.9003 \t Acc 80.73 \t AccHead 82.21 \t AccTail 79.25\n",
      "Epoch: [033] \t Loss 0.8978 \t Acc 78.96 \t AccHead 81.04 \t AccTail 76.89\n",
      "Epoch: [034] \t Loss 0.8658 \t Acc 79.08 \t AccHead 82.21 \t AccTail 75.94\n",
      "Epoch: [035] \t Loss 0.8427 \t Acc 78.38 \t AccHead 81.53 \t AccTail 75.24\n",
      "Epoch: [036] \t Loss 0.8164 \t Acc 79.85 \t AccHead 82.58 \t AccTail 77.12\n",
      "Epoch: [037] \t Loss 0.8315 \t Acc 80.78 \t AccHead 83.01 \t AccTail 78.54\n",
      "Epoch: [038] \t Loss 0.7916 \t Acc 81.28 \t AccHead 83.32 \t AccTail 79.25\n",
      "Epoch: [039] \t Loss 0.7863 \t Acc 80.54 \t AccHead 83.01 \t AccTail 78.07\n",
      "Epoch: [040] \t Loss 0.8022 \t Acc 79.69 \t AccHead 82.03 \t AccTail 77.36\n",
      "Epoch: [041] \t Loss 0.7763 \t Acc 78.84 \t AccHead 81.04 \t AccTail 76.65\n",
      "Epoch: [042] \t Loss 0.7644 \t Acc 80.90 \t AccHead 84.68 \t AccTail 77.12\n",
      "Epoch: [043] \t Loss 0.7737 \t Acc 80.69 \t AccHead 83.32 \t AccTail 78.07\n",
      "Epoch: [044] \t Loss 0.7358 \t Acc 81.24 \t AccHead 84.19 \t AccTail 78.30\n",
      "Epoch: [045] \t Loss 0.7299 \t Acc 81.80 \t AccHead 83.88 \t AccTail 79.72\n",
      "Epoch: [046] \t Loss 0.7214 \t Acc 80.43 \t AccHead 81.84 \t AccTail 79.01\n",
      "Epoch: [047] \t Loss 0.7172 \t Acc 79.55 \t AccHead 80.79 \t AccTail 78.30\n",
      "Epoch: [048] \t Loss 0.7208 \t Acc 78.30 \t AccHead 83.01 \t AccTail 73.58\n",
      "Epoch: [049] \t Loss 0.7149 \t Acc 78.62 \t AccHead 82.71 \t AccTail 74.53\n",
      "Epoch: [050] \t Loss 0.6823 \t Acc 82.12 \t AccHead 82.64 \t AccTail 81.60\n",
      "Epoch: [051] \t Loss 0.6850 \t Acc 81.09 \t AccHead 83.88 \t AccTail 78.30\n",
      "Epoch: [052] \t Loss 0.6845 \t Acc 80.42 \t AccHead 84.19 \t AccTail 76.65\n",
      "Epoch: [053] \t Loss 0.6814 \t Acc 80.36 \t AccHead 84.31 \t AccTail 76.42\n",
      "Epoch: [054] \t Loss 0.6758 \t Acc 81.64 \t AccHead 83.57 \t AccTail 79.72\n",
      "Epoch: [055] \t Loss 0.6965 \t Acc 80.84 \t AccHead 83.14 \t AccTail 78.54\n",
      "Epoch: [056] \t Loss 0.6656 \t Acc 80.55 \t AccHead 82.33 \t AccTail 78.77\n",
      "Epoch: [057] \t Loss 0.6634 \t Acc 81.88 \t AccHead 84.74 \t AccTail 79.01\n",
      "Epoch: [058] \t Loss 0.6346 \t Acc 81.59 \t AccHead 83.69 \t AccTail 79.48\n",
      "Epoch: [059] \t Loss 0.6283 \t Acc 82.19 \t AccHead 84.43 \t AccTail 79.95\n",
      "Epoch: [060] \t Loss 0.6343 \t Acc 83.22 \t AccHead 84.37 \t AccTail 82.08\n",
      "Epoch: [061] \t Loss 0.6306 \t Acc 81.23 \t AccHead 83.45 \t AccTail 79.01\n",
      "Epoch: [062] \t Loss 0.6223 \t Acc 83.37 \t AccHead 84.43 \t AccTail 82.31\n",
      "Epoch: [063] \t Loss 0.5912 \t Acc 79.75 \t AccHead 80.48 \t AccTail 79.01\n",
      "Epoch: [064] \t Loss 0.6240 \t Acc 81.19 \t AccHead 81.72 \t AccTail 80.66\n",
      "Epoch: [065] \t Loss 0.6098 \t Acc 79.26 \t AccHead 84.93 \t AccTail 73.58\n",
      "Epoch: [066] \t Loss 0.6302 \t Acc 81.03 \t AccHead 82.58 \t AccTail 79.48\n",
      "Epoch: [067] \t Loss 0.5931 \t Acc 81.08 \t AccHead 84.81 \t AccTail 77.36\n",
      "Epoch: [068] \t Loss 0.5919 \t Acc 83.08 \t AccHead 84.56 \t AccTail 81.60\n",
      "Epoch: [069] \t Loss 0.5876 \t Acc 80.97 \t AccHead 83.63 \t AccTail 78.30\n",
      "Epoch: [070] \t Loss 0.5849 \t Acc 79.40 \t AccHead 83.57 \t AccTail 75.24\n",
      "Epoch: [071] \t Loss 0.5663 \t Acc 81.76 \t AccHead 84.74 \t AccTail 78.77\n",
      "Epoch: [072] \t Loss 0.5694 \t Acc 83.47 \t AccHead 84.87 \t AccTail 82.08\n",
      "Epoch: [073] \t Loss 0.5745 \t Acc 83.39 \t AccHead 83.76 \t AccTail 83.02\n",
      "Epoch: [074] \t Loss 0.5627 \t Acc 80.58 \t AccHead 83.32 \t AccTail 77.83\n",
      "Epoch: [075] \t Loss 0.5818 \t Acc 82.40 \t AccHead 82.95 \t AccTail 81.84\n",
      "Epoch: [076] \t Loss 0.5578 \t Acc 81.92 \t AccHead 84.13 \t AccTail 79.72\n",
      "Epoch: [077] \t Loss 0.5607 \t Acc 82.46 \t AccHead 83.32 \t AccTail 81.60\n",
      "Epoch: [078] \t Loss 0.5587 \t Acc 83.30 \t AccHead 85.24 \t AccTail 81.37\n",
      "Epoch: [079] \t Loss 0.5314 \t Acc 82.03 \t AccHead 83.88 \t AccTail 80.19\n",
      "Epoch: [080] \t Loss 0.5683 \t Acc 82.85 \t AccHead 84.56 \t AccTail 81.13\n",
      "Epoch: [081] \t Loss 0.5454 \t Acc 84.40 \t AccHead 84.13 \t AccTail 84.67\n",
      "Epoch: [082] \t Loss 0.5304 \t Acc 81.76 \t AccHead 83.57 \t AccTail 79.95\n",
      "Epoch: [083] \t Loss 0.5364 \t Acc 82.64 \t AccHead 84.62 \t AccTail 80.66\n",
      "Epoch: [084] \t Loss 0.5205 \t Acc 82.48 \t AccHead 85.24 \t AccTail 79.72\n",
      "Epoch: [085] \t Loss 0.5332 \t Acc 83.66 \t AccHead 85.48 \t AccTail 81.84\n",
      "Epoch: [086] \t Loss 0.5120 \t Acc 83.78 \t AccHead 85.48 \t AccTail 82.08\n",
      "Epoch: [087] \t Loss 0.5190 \t Acc 83.65 \t AccHead 84.99 \t AccTail 82.31\n",
      "Epoch: [088] \t Loss 0.5447 \t Acc 81.69 \t AccHead 84.37 \t AccTail 79.01\n",
      "Epoch: [089] \t Loss 0.4917 \t Acc 81.38 \t AccHead 83.76 \t AccTail 79.01\n",
      "Epoch: [090] \t Loss 0.5062 \t Acc 83.78 \t AccHead 85.48 \t AccTail 82.08\n",
      "Epoch: [091] \t Loss 0.5166 \t Acc 81.60 \t AccHead 84.43 \t AccTail 78.77\n",
      "Epoch: [092] \t Loss 0.5196 \t Acc 83.46 \t AccHead 85.55 \t AccTail 81.37\n",
      "Epoch: [093] \t Loss 0.5118 \t Acc 82.51 \t AccHead 85.55 \t AccTail 79.48\n",
      "Epoch: [094] \t Loss 0.5069 \t Acc 81.74 \t AccHead 85.42 \t AccTail 78.07\n",
      "Epoch: [095] \t Loss 0.5071 \t Acc 82.45 \t AccHead 82.58 \t AccTail 82.31\n",
      "Epoch: [096] \t Loss 0.5020 \t Acc 83.33 \t AccHead 85.05 \t AccTail 81.60\n",
      "Epoch: [097] \t Loss 0.5126 \t Acc 81.90 \t AccHead 83.38 \t AccTail 80.42\n",
      "Epoch: [098] \t Loss 0.4980 \t Acc 82.60 \t AccHead 84.06 \t AccTail 81.13\n",
      "Epoch: [099] \t Loss 0.4971 \t Acc 84.28 \t AccHead 85.55 \t AccTail 83.02\n",
      "Epoch: [100] \t Loss 0.4848 \t Acc 83.06 \t AccHead 83.57 \t AccTail 82.55\n"
     ]
    }
   ],
   "source": [
    "print('STEP 7: TRAIN THE MODEL')\n",
    "cri=num_classes//2-1\n",
    "max_norm = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_history = []\n",
    "    # train\n",
    "    model_head.train()\n",
    "    model_tail.train()\n",
    "    for batch_index, data in enumerate(train_loader):\n",
    "        image, target = data\n",
    "        image, target = image.cuda(), target.cuda()\n",
    "        \n",
    "        image_tail = image[target>cri]\n",
    "        target_tail = target[target>cri]\n",
    "        image_head = image[target<=cri]\n",
    "        target_head = target[target<=cri]\n",
    "            \n",
    "        pred_head = model_head(image_head)\n",
    "        loss_head = criterion(pred_head, target_head)\n",
    "        optimizer_head.zero_grad()\n",
    "        loss_head.backward()\n",
    "        optimizer_head.step()\n",
    "        \n",
    "        if target_tail.size()[0] == 1 :\n",
    "            image_tail=torch.cat([image_tail, image_tail])\n",
    "            target_tail = torch.cat([target_tail, target_tail])\n",
    "            \n",
    "        if target_tail.size()[0] == 0 :\n",
    "            loss = loss_head\n",
    "#             print('ok')\n",
    "        else :\n",
    "            pred_tail = model_tail(image_tail)\n",
    "            loss_tail = criterion(pred_tail, target_tail)\n",
    "            optimizer_tail.zero_grad()\n",
    "            optimizer_tail.zero_grad()\n",
    "            loss_tail.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model_tail.parameters(), max_norm)\n",
    "            optimizer_tail.step()     \n",
    "            loss = loss_head+loss_tail\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "        \n",
    "    # eval\n",
    "    topk_acc, head_acc_ok, tail_acc = compute_accuracy(val_loader, model_head)\n",
    "    topk_acc, head_acc, tail_acc_ok = compute_accuracy(val_loader, model_tail)\n",
    "    \n",
    "    topk_acc = (head_acc_ok[0]+ tail_acc_ok[0])/2\n",
    "    tail_acc = tail_acc_ok[0]\n",
    "    head_acc = head_acc_ok[0]\n",
    "    \n",
    "    loss_mean = np.mean(loss_history)\n",
    "    scheduler_head.step()\n",
    "    scheduler_tail.step()\n",
    "\n",
    "    print('Epoch: [{:03d}] \\t Loss {:.4f} \\t Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(epoch+1, loss_mean, topk_acc, head_acc, tail_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ebfa934f-3869-4519-86e1-abd9173b62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "55cc4868",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_head': model_head.state_dict(),\n",
    "    'optimizer_head': optimizer_head.state_dict(),\n",
    "    'model_tail': model_tail.state_dict(),\n",
    "    'optimizer_tail': optimizer_tail.state_dict(),\n",
    "    'epoch': epoch},\n",
    "    osp.join(SAVE_DIR, 'ep{:03d}.pth'.format(epoch+1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e572b222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 80.36 \t AccHead 80.56 \t AccTail 80.16\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = 'logs/Tendency/mymodel/CIFAR10/exp-0.1' \n",
    "model_head = ResNet18(num_classes)\n",
    "model_tail = ResNet18(num_classes)\n",
    "model_head.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model_head'])\n",
    "model_tail.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model_tail'])\n",
    "model_head = model_head.cuda()\n",
    "model_tail = model_tail.cuda()\n",
    "\n",
    "topk_acc, head_acc_ok, tail_acc = compute_accuracy(test_loader, model_head)\n",
    "topk_acc, head_acc, tail_acc_ok = compute_accuracy(test_loader, model_tail)\n",
    "topk_acc = (head_acc_ok[0]+ tail_acc_ok[0])/2\n",
    "tail_acc = tail_acc_ok[0]\n",
    "head_acc = head_acc_ok[0]\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc, head_acc, tail_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a9d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
