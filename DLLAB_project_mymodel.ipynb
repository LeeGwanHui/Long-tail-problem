{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be564ffe-7c6a-4983-a927-c53f79fe530b",
   "metadata": {},
   "source": [
    "> ### EEE4423: Deep Learning Lab\n",
    "\n",
    "# Final Project: Long-tail Visual Recognition for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d8d4f-d166-4203-9223-f5b675b2ebd8",
   "metadata": {},
   "source": [
    "<h4><div style=\"text-align: right\"> Due date: June 24, 2022.  </div> <br>\n",
    "<div style=\"text-align: right\"> Please upload your file @ LearnUs and submit via e-mail by 2 PM in the form of [ID_Name_project.ipynb]. </div></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00411697",
   "metadata": {},
   "source": [
    "여기서는 두가지 방법을 사용할 것이다.\n",
    "\n",
    "1) model을 head에 해당하는 곳과 tail에 해당하는 곳 두군대 모두 구한다.\n",
    "\n",
    "2) LWS 기법을 각각 사용해서 정확도를 높인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0d7d3b2-32a4-4808-9476-edc89d829705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from misc.project.utils import resnet18, IMBALANCECIFAR10, IMBALANCECIFAR100, compute_accuracy\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) ## 경고 무시\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03866650-32bc-4868-bcd7-eca0888adda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6: INSTANTIATE OPTIMIZER CLASS\n"
     ]
    }
   ],
   "source": [
    "print('STEP 6: INSTANTIATE OPTIMIZER CLASS')\n",
    "\n",
    "DATASET = 'CIFAR10' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'exp' #['exp', 'step'] # 질문\n",
    "IMB_FACTOR = 0.1 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/final/CIFAR10/exp-0.1' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128 \n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 2e-4\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebfa934f-3869-4519-86e1-abd9173b62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "665d1d9f-ea6a-4d00-bd1e-e42d28565546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: LOADING DATASET\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[5000, 3871, 2997, 2320, 1796, 1391, 1077, 834, 645, 500]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print('STEP 1: LOADING DATASET')\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "if DATASET == 'CIFAR10':\n",
    "    train_dataset_0 = IMBALANCECIFAR10(root='../dataset/project', imb_type=IMB_TYPE, imb_factor=IMB_FACTOR, train=True, download=True, transform=transform_train)\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root='../dataset/project', train=False, download=True, transform=transform_test)\n",
    "elif DATASET == 'CIFAR100':\n",
    "    train_dataset_0 = IMBALANCECIFAR100(root='../dataset/project', imb_type=IMB_TYPE, imb_factor=IMB_FACTOR, train=True, download=True, transform=transform_train)\n",
    "    test_dataset = torchvision.datasets.CIFAR100(root='../dataset/project', train=False, download=True, transform=transform_test)\n",
    "\n",
    "cls_num_list = train_dataset_0.get_cls_num_list() # 각 class에 몇개가 들어있는지 나타냄 \n",
    "print('cls num list:')\n",
    "print(cls_num_list)\n",
    "num_classes = len(cls_num_list)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60d43001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500, 387, 299, 232, 179, 139, 107, 83, 64, 50]\n",
      "[4500, 3484, 2698, 2088, 1617, 1252, 970, 751, 581, 450]\n"
     ]
    }
   ],
   "source": [
    "list_val = []\n",
    "for i in cls_num_list :\n",
    "    if i < 10 :\n",
    "        k = 1\n",
    "        list_val.append(k)\n",
    "    elif i >= 10 :\n",
    "        k = int(i*0.1)\n",
    "        list_val.append(k)\n",
    "print(list_val)\n",
    "list_v_t = []\n",
    "for i in range(num_classes) :\n",
    "    list_v_t.append(cls_num_list[i] - list_val[i])\n",
    "print(list_v_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9abfba5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20431\n",
      "18388\n",
      "2043\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "torch.manual_seed(torch.initial_seed())\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(0)\n",
    "dataset_size = len(train_dataset_0)\n",
    "validation_size = int(dataset_size * 0.1) if not int(dataset_size * 0.1)==0 else 1 \n",
    "train_size = dataset_size - validation_size\n",
    "test_size = 0\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset_0 = random_split(train_dataset_0, [train_size, validation_size, test_size],generator=generator)\n",
    "\n",
    "print(len(train_dataset_0))\n",
    "print(len(train_dataset))\n",
    "print(len(validation_dataset))\n",
    "print(len(test_dataset_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad851470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: MAKING DATASET ITERABLE\n"
     ]
    }
   ],
   "source": [
    "print('STEP 2: MAKING DATASET ITERABLE')\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=4, drop_last=False)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    validation_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=4, drop_last=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=100, shuffle=False,\n",
    "    num_workers=4, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1a22399-4ce8-4320-ab5c-813fdfa045d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: CREATE MODEL CLASS (VGG16)\n"
     ]
    }
   ],
   "source": [
    "print('STEP 3: CREATE MODEL CLASS (VGG16)')\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.backbone = resnet18()\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "        self.scales = Parameter(torch.ones(num_classes))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone(x)\n",
    "        x = F.adaptive_max_pool2d(x, 1)\n",
    "        x = x.view(batch_size, -1)\n",
    "        pred = self.classifier(x)\n",
    "        # LWS 구현\n",
    "        pred *= self.scales\n",
    "#         print(pred.size()) # torch.Size([128, 10])\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a526c9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: INSTANTIATE MODEL CLASS\n",
      "STEP 5: INSTANTIATE LOSS CLASS\n"
     ]
    }
   ],
   "source": [
    "print('STEP 4: INSTANTIATE MODEL CLASS')\n",
    "\n",
    "model_head = ResNet18(num_classes)\n",
    "model_tail = ResNet18(num_classes)\n",
    "model_head = model_head.cuda()\n",
    "model_tail = model_tail.cuda()\n",
    "\n",
    "print('STEP 5: INSTANTIATE LOSS CLASS')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_head = torch.optim.SGD(model_head.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "optimizer_tail = torch.optim.SGD(model_tail.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler_head = torch.optim.lr_scheduler.StepLR(optimizer_head, step_size=150, gamma=0.1)\n",
    "scheduler_tail = torch.optim.lr_scheduler.StepLR(optimizer_tail, step_size=150, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef63542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54e43dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 7: TRAIN THE MODEL\n",
      "Epoch: [001] \t Loss 8.4077 \t Acc 9.22 \t AccHead 11.30 \t AccTail 7.14\n",
      "Epoch: [002] \t Loss 6.9037 \t Acc 13.93 \t AccHead 16.39 \t AccTail 11.47\n",
      "Epoch: [003] \t Loss 6.6006 \t Acc 13.81 \t AccHead 16.79 \t AccTail 10.82\n",
      "Epoch: [004] \t Loss 6.3439 \t Acc 18.07 \t AccHead 23.14 \t AccTail 12.99\n",
      "Epoch: [005] \t Loss 6.1162 \t Acc 21.33 \t AccHead 24.48 \t AccTail 18.18\n",
      "Epoch: [006] \t Loss 5.9234 \t Acc 20.84 \t AccHead 27.83 \t AccTail 13.85\n",
      "Epoch: [007] \t Loss 5.7197 \t Acc 21.45 \t AccHead 27.09 \t AccTail 15.80\n",
      "Epoch: [008] \t Loss 5.5644 \t Acc 24.35 \t AccHead 30.30 \t AccTail 18.40\n",
      "Epoch: [009] \t Loss 5.3618 \t Acc 25.02 \t AccHead 30.57 \t AccTail 19.48\n",
      "Epoch: [010] \t Loss 5.1676 \t Acc 27.57 \t AccHead 31.77 \t AccTail 23.38\n",
      "Epoch: [011] \t Loss 5.0363 \t Acc 27.87 \t AccHead 35.18 \t AccTail 20.56\n",
      "Epoch: [012] \t Loss 4.9065 \t Acc 28.90 \t AccHead 34.65 \t AccTail 23.16\n",
      "Epoch: [013] \t Loss 4.7776 \t Acc 30.89 \t AccHead 38.19 \t AccTail 23.59\n",
      "Epoch: [014] \t Loss 4.6570 \t Acc 32.63 \t AccHead 39.93 \t AccTail 25.32\n",
      "Epoch: [015] \t Loss 4.5366 \t Acc 32.14 \t AccHead 38.73 \t AccTail 25.54\n",
      "Epoch: [016] \t Loss 4.4432 \t Acc 32.88 \t AccHead 37.19 \t AccTail 28.57\n",
      "Epoch: [017] \t Loss 4.3442 \t Acc 33.39 \t AccHead 39.73 \t AccTail 27.06\n",
      "Epoch: [018] \t Loss 4.2568 \t Acc 33.83 \t AccHead 38.86 \t AccTail 28.79\n",
      "Epoch: [019] \t Loss 4.1424 \t Acc 35.81 \t AccHead 43.48 \t AccTail 28.14\n",
      "Epoch: [020] \t Loss 4.0558 \t Acc 35.70 \t AccHead 42.61 \t AccTail 28.79\n",
      "Epoch: [021] \t Loss 3.9620 \t Acc 35.05 \t AccHead 40.67 \t AccTail 29.44\n",
      "Epoch: [022] \t Loss 3.8976 \t Acc 36.30 \t AccHead 42.94 \t AccTail 29.65\n",
      "Epoch: [023] \t Loss 3.8307 \t Acc 34.42 \t AccHead 42.01 \t AccTail 26.84\n",
      "Epoch: [024] \t Loss 3.7239 \t Acc 35.87 \t AccHead 41.87 \t AccTail 29.87\n",
      "Epoch: [025] \t Loss 3.6664 \t Acc 37.20 \t AccHead 41.07 \t AccTail 33.33\n",
      "Epoch: [026] \t Loss 3.5628 \t Acc 38.06 \t AccHead 44.08 \t AccTail 32.03\n",
      "Epoch: [027] \t Loss 3.4847 \t Acc 35.73 \t AccHead 43.75 \t AccTail 27.71\n",
      "Epoch: [028] \t Loss 3.4888 \t Acc 37.99 \t AccHead 43.08 \t AccTail 32.90\n",
      "Epoch: [029] \t Loss 3.3556 \t Acc 38.03 \t AccHead 44.68 \t AccTail 31.39\n",
      "Epoch: [030] \t Loss 3.3272 \t Acc 38.05 \t AccHead 43.41 \t AccTail 32.68\n",
      "Epoch: [031] \t Loss 3.2587 \t Acc 38.87 \t AccHead 45.48 \t AccTail 32.25\n",
      "Epoch: [032] \t Loss 3.1822 \t Acc 37.88 \t AccHead 44.82 \t AccTail 30.95\n",
      "Epoch: [033] \t Loss 3.0957 \t Acc 39.38 \t AccHead 45.42 \t AccTail 33.33\n",
      "Epoch: [034] \t Loss 3.0545 \t Acc 37.76 \t AccHead 43.28 \t AccTail 32.25\n",
      "Epoch: [035] \t Loss 3.0136 \t Acc 37.89 \t AccHead 43.75 \t AccTail 32.03\n",
      "Epoch: [036] \t Loss 2.9361 \t Acc 39.05 \t AccHead 45.42 \t AccTail 32.68\n",
      "Epoch: [037] \t Loss 2.9248 \t Acc 38.42 \t AccHead 43.95 \t AccTail 32.90\n",
      "Epoch: [038] \t Loss 2.8533 \t Acc 39.46 \t AccHead 46.02 \t AccTail 32.90\n",
      "Epoch: [039] \t Loss 2.8464 \t Acc 41.51 \t AccHead 47.09 \t AccTail 35.93\n",
      "Epoch: [040] \t Loss 2.7723 \t Acc 38.77 \t AccHead 42.47 \t AccTail 35.06\n",
      "Epoch: [041] \t Loss 2.7161 \t Acc 41.19 \t AccHead 46.02 \t AccTail 36.36\n",
      "Epoch: [042] \t Loss 2.6793 \t Acc 40.37 \t AccHead 45.02 \t AccTail 35.71\n",
      "Epoch: [043] \t Loss 2.6486 \t Acc 39.45 \t AccHead 44.48 \t AccTail 34.42\n",
      "Epoch: [044] \t Loss 2.5519 \t Acc 41.40 \t AccHead 45.35 \t AccTail 37.45\n",
      "Epoch: [045] \t Loss 2.5556 \t Acc 40.50 \t AccHead 43.55 \t AccTail 37.45\n",
      "Epoch: [046] \t Loss 2.4961 \t Acc 39.21 \t AccHead 43.14 \t AccTail 35.28\n",
      "Epoch: [047] \t Loss 2.4486 \t Acc 39.97 \t AccHead 45.95 \t AccTail 33.98\n",
      "Epoch: [048] \t Loss 2.4230 \t Acc 39.37 \t AccHead 42.81 \t AccTail 35.93\n",
      "Epoch: [049] \t Loss 2.4002 \t Acc 40.90 \t AccHead 45.22 \t AccTail 36.58\n",
      "Epoch: [050] \t Loss 2.3553 \t Acc 40.87 \t AccHead 44.95 \t AccTail 36.80\n",
      "Epoch: [051] \t Loss 2.2451 \t Acc 40.62 \t AccHead 45.08 \t AccTail 36.15\n",
      "Epoch: [052] \t Loss 2.2579 \t Acc 40.34 \t AccHead 46.69 \t AccTail 33.98\n",
      "Epoch: [053] \t Loss 2.2667 \t Acc 41.29 \t AccHead 46.22 \t AccTail 36.36\n",
      "Epoch: [054] \t Loss 2.1454 \t Acc 41.54 \t AccHead 47.16 \t AccTail 35.93\n",
      "Epoch: [055] \t Loss 2.1829 \t Acc 40.12 \t AccHead 45.82 \t AccTail 34.42\n",
      "Epoch: [056] \t Loss 2.2024 \t Acc 42.13 \t AccHead 47.89 \t AccTail 36.36\n",
      "Epoch: [057] \t Loss 2.0950 \t Acc 40.31 \t AccHead 45.55 \t AccTail 35.06\n",
      "Epoch: [058] \t Loss 2.0334 \t Acc 43.40 \t AccHead 45.89 \t AccTail 40.91\n",
      "Epoch: [059] \t Loss 2.0672 \t Acc 40.37 \t AccHead 45.89 \t AccTail 34.85\n",
      "Epoch: [060] \t Loss 2.0488 \t Acc 42.78 \t AccHead 47.89 \t AccTail 37.66\n",
      "Epoch: [061] \t Loss 1.9379 \t Acc 40.32 \t AccHead 45.35 \t AccTail 35.28\n",
      "Epoch: [062] \t Loss 2.0169 \t Acc 39.79 \t AccHead 44.95 \t AccTail 34.63\n",
      "Epoch: [063] \t Loss 1.9479 \t Acc 39.92 \t AccHead 45.42 \t AccTail 34.42\n",
      "Epoch: [064] \t Loss 1.8901 \t Acc 40.23 \t AccHead 44.75 \t AccTail 35.71\n",
      "Epoch: [065] \t Loss 1.9753 \t Acc 40.35 \t AccHead 46.29 \t AccTail 34.42\n",
      "Epoch: [066] \t Loss 1.9033 \t Acc 42.17 \t AccHead 44.95 \t AccTail 39.39\n",
      "Epoch: [067] \t Loss 1.8845 \t Acc 41.21 \t AccHead 46.49 \t AccTail 35.93\n",
      "Epoch: [068] \t Loss 1.8864 \t Acc 41.63 \t AccHead 46.69 \t AccTail 36.58\n",
      "Epoch: [069] \t Loss 1.8321 \t Acc 41.12 \t AccHead 45.02 \t AccTail 37.23\n",
      "Epoch: [070] \t Loss 1.7950 \t Acc 41.79 \t AccHead 46.35 \t AccTail 37.23\n",
      "Epoch: [071] \t Loss 1.7982 \t Acc 39.65 \t AccHead 47.49 \t AccTail 31.82\n",
      "Epoch: [072] \t Loss 1.8009 \t Acc 39.30 \t AccHead 44.62 \t AccTail 33.98\n",
      "Epoch: [073] \t Loss 1.7570 \t Acc 41.60 \t AccHead 46.62 \t AccTail 36.58\n",
      "Epoch: [074] \t Loss 1.7240 \t Acc 41.73 \t AccHead 47.96 \t AccTail 35.50\n",
      "Epoch: [075] \t Loss 1.7205 \t Acc 41.37 \t AccHead 46.15 \t AccTail 36.58\n",
      "Epoch: [076] \t Loss 1.6438 \t Acc 42.81 \t AccHead 47.96 \t AccTail 37.66\n",
      "Epoch: [077] \t Loss 1.6763 \t Acc 42.60 \t AccHead 46.02 \t AccTail 39.18\n",
      "Epoch: [078] \t Loss 1.6647 \t Acc 40.81 \t AccHead 43.75 \t AccTail 37.88\n",
      "Epoch: [079] \t Loss 1.6162 \t Acc 40.92 \t AccHead 46.56 \t AccTail 35.28\n",
      "Epoch: [080] \t Loss 1.5975 \t Acc 42.80 \t AccHead 46.42 \t AccTail 39.18\n",
      "Epoch: [081] \t Loss 1.6715 \t Acc 42.13 \t AccHead 47.02 \t AccTail 37.23\n",
      "Epoch: [082] \t Loss 1.5830 \t Acc 41.84 \t AccHead 47.76 \t AccTail 35.93\n",
      "Epoch: [083] \t Loss 1.5456 \t Acc 40.88 \t AccHead 45.62 \t AccTail 36.15\n",
      "Epoch: [084] \t Loss 1.5859 \t Acc 41.80 \t AccHead 46.15 \t AccTail 37.45\n",
      "Epoch: [085] \t Loss 1.5541 \t Acc 41.87 \t AccHead 46.09 \t AccTail 37.66\n",
      "Epoch: [086] \t Loss 1.5416 \t Acc 39.74 \t AccHead 46.35 \t AccTail 33.12\n",
      "Epoch: [087] \t Loss 1.5355 \t Acc 40.85 \t AccHead 46.42 \t AccTail 35.28\n",
      "Epoch: [088] \t Loss 1.5011 \t Acc 40.50 \t AccHead 47.22 \t AccTail 33.77\n",
      "Epoch: [089] \t Loss 1.4810 \t Acc 41.38 \t AccHead 46.62 \t AccTail 36.15\n",
      "Epoch: [090] \t Loss 1.5076 \t Acc 41.63 \t AccHead 45.82 \t AccTail 37.45\n",
      "Epoch: [091] \t Loss 1.4915 \t Acc 42.71 \t AccHead 46.89 \t AccTail 38.53\n",
      "Epoch: [092] \t Loss 1.4676 \t Acc 43.03 \t AccHead 46.02 \t AccTail 40.04\n",
      "Epoch: [093] \t Loss 1.4698 \t Acc 41.35 \t AccHead 47.63 \t AccTail 35.06\n",
      "Epoch: [094] \t Loss 1.4645 \t Acc 39.29 \t AccHead 44.82 \t AccTail 33.77\n",
      "Epoch: [095] \t Loss 1.4082 \t Acc 43.46 \t AccHead 46.02 \t AccTail 40.91\n",
      "Epoch: [096] \t Loss 1.4474 \t Acc 40.80 \t AccHead 45.89 \t AccTail 35.71\n",
      "Epoch: [097] \t Loss 1.4249 \t Acc 41.13 \t AccHead 45.69 \t AccTail 36.58\n",
      "Epoch: [098] \t Loss 1.3802 \t Acc 41.99 \t AccHead 45.02 \t AccTail 38.96\n",
      "Epoch: [099] \t Loss 1.3855 \t Acc 40.56 \t AccHead 44.55 \t AccTail 36.58\n",
      "Epoch: [100] \t Loss 1.3885 \t Acc 41.37 \t AccHead 42.47 \t AccTail 40.26\n",
      "Epoch: [101] \t Loss 1.3810 \t Acc 40.33 \t AccHead 46.02 \t AccTail 34.63\n",
      "Epoch: [102] \t Loss 1.4063 \t Acc 40.94 \t AccHead 45.95 \t AccTail 35.93\n",
      "Epoch: [103] \t Loss 1.3658 \t Acc 42.94 \t AccHead 47.56 \t AccTail 38.31\n",
      "Epoch: [104] \t Loss 1.3375 \t Acc 39.18 \t AccHead 44.82 \t AccTail 33.55\n",
      "Epoch: [105] \t Loss 1.3531 \t Acc 41.12 \t AccHead 45.22 \t AccTail 37.01\n",
      "Epoch: [106] \t Loss 1.3845 \t Acc 40.47 \t AccHead 45.02 \t AccTail 35.93\n",
      "Epoch: [107] \t Loss 1.3423 \t Acc 40.09 \t AccHead 47.49 \t AccTail 32.68\n",
      "Epoch: [108] \t Loss 1.3120 \t Acc 39.82 \t AccHead 46.09 \t AccTail 33.55\n",
      "Epoch: [109] \t Loss 1.3409 \t Acc 42.24 \t AccHead 45.08 \t AccTail 39.39\n",
      "Epoch: [110] \t Loss 1.2545 \t Acc 40.77 \t AccHead 46.69 \t AccTail 34.85\n",
      "Epoch: [111] \t Loss 1.2507 \t Acc 39.57 \t AccHead 46.02 \t AccTail 33.12\n",
      "Epoch: [112] \t Loss 1.3145 \t Acc 42.11 \t AccHead 47.42 \t AccTail 36.80\n",
      "Epoch: [113] \t Loss 1.3146 \t Acc 42.21 \t AccHead 47.63 \t AccTail 36.80\n",
      "Epoch: [114] \t Loss 1.2465 \t Acc 41.37 \t AccHead 46.15 \t AccTail 36.58\n",
      "Epoch: [115] \t Loss 1.3075 \t Acc 41.38 \t AccHead 46.82 \t AccTail 35.93\n",
      "Epoch: [116] \t Loss 1.2120 \t Acc 41.24 \t AccHead 45.69 \t AccTail 36.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [117] \t Loss 1.2859 \t Acc 42.05 \t AccHead 47.96 \t AccTail 36.15\n",
      "Epoch: [118] \t Loss 1.2392 \t Acc 41.88 \t AccHead 47.83 \t AccTail 35.93\n",
      "Epoch: [119] \t Loss 1.2544 \t Acc 40.03 \t AccHead 45.22 \t AccTail 34.85\n",
      "Epoch: [120] \t Loss 1.2328 \t Acc 42.40 \t AccHead 46.49 \t AccTail 38.31\n",
      "Epoch: [121] \t Loss 1.2029 \t Acc 41.00 \t AccHead 46.29 \t AccTail 35.71\n",
      "Epoch: [122] \t Loss 1.1700 \t Acc 42.28 \t AccHead 47.76 \t AccTail 36.80\n",
      "Epoch: [123] \t Loss 1.2473 \t Acc 41.08 \t AccHead 46.02 \t AccTail 36.15\n",
      "Epoch: [124] \t Loss 1.1719 \t Acc 41.09 \t AccHead 45.82 \t AccTail 36.36\n",
      "Epoch: [125] \t Loss 1.1697 \t Acc 41.48 \t AccHead 47.89 \t AccTail 35.06\n",
      "Epoch: [126] \t Loss 1.2424 \t Acc 43.72 \t AccHead 46.96 \t AccTail 40.48\n",
      "Epoch: [127] \t Loss 1.2323 \t Acc 40.88 \t AccHead 43.88 \t AccTail 37.88\n",
      "Epoch: [128] \t Loss 1.1975 \t Acc 41.86 \t AccHead 46.49 \t AccTail 37.23\n",
      "Epoch: [129] \t Loss 1.1933 \t Acc 41.36 \t AccHead 48.09 \t AccTail 34.63\n",
      "Epoch: [130] \t Loss 1.1803 \t Acc 41.29 \t AccHead 46.22 \t AccTail 36.36\n",
      "Epoch: [131] \t Loss 1.1702 \t Acc 41.81 \t AccHead 47.69 \t AccTail 35.93\n",
      "Epoch: [132] \t Loss 1.2220 \t Acc 40.73 \t AccHead 44.88 \t AccTail 36.58\n",
      "Epoch: [133] \t Loss 1.1692 \t Acc 42.73 \t AccHead 47.16 \t AccTail 38.31\n",
      "Epoch: [134] \t Loss 1.1774 \t Acc 41.51 \t AccHead 47.09 \t AccTail 35.93\n",
      "Epoch: [135] \t Loss 1.2025 \t Acc 40.52 \t AccHead 45.75 \t AccTail 35.28\n",
      "Epoch: [136] \t Loss 1.1700 \t Acc 43.88 \t AccHead 48.36 \t AccTail 39.39\n",
      "Epoch: [137] \t Loss 1.1580 \t Acc 41.56 \t AccHead 45.02 \t AccTail 38.10\n",
      "Epoch: [138] \t Loss 1.1677 \t Acc 40.69 \t AccHead 45.02 \t AccTail 36.36\n",
      "Epoch: [139] \t Loss 1.1630 \t Acc 40.58 \t AccHead 46.96 \t AccTail 34.20\n",
      "Epoch: [140] \t Loss 1.1761 \t Acc 40.47 \t AccHead 46.09 \t AccTail 34.85\n",
      "Epoch: [141] \t Loss 1.1267 \t Acc 40.54 \t AccHead 45.15 \t AccTail 35.93\n",
      "Epoch: [142] \t Loss 1.2043 \t Acc 39.61 \t AccHead 45.89 \t AccTail 33.33\n",
      "Epoch: [143] \t Loss 1.1622 \t Acc 40.88 \t AccHead 46.69 \t AccTail 35.06\n",
      "Epoch: [144] \t Loss 1.2017 \t Acc 41.61 \t AccHead 45.55 \t AccTail 37.66\n",
      "Epoch: [145] \t Loss 1.1722 \t Acc 42.94 \t AccHead 48.23 \t AccTail 37.66\n",
      "Epoch: [146] \t Loss 1.0909 \t Acc 41.31 \t AccHead 45.82 \t AccTail 36.80\n",
      "Epoch: [147] \t Loss 1.2125 \t Acc 42.02 \t AccHead 46.15 \t AccTail 37.88\n",
      "Epoch: [148] \t Loss 1.1040 \t Acc 40.13 \t AccHead 46.49 \t AccTail 33.77\n",
      "Epoch: [149] \t Loss 1.1250 \t Acc 39.18 \t AccHead 45.89 \t AccTail 32.47\n",
      "Epoch: [150] \t Loss 1.1577 \t Acc 42.05 \t AccHead 47.09 \t AccTail 37.01\n",
      "Epoch: [151] \t Loss 0.5655 \t Acc 46.73 \t AccHead 51.91 \t AccTail 41.56\n",
      "Epoch: [152] \t Loss 0.3221 \t Acc 47.97 \t AccHead 52.64 \t AccTail 43.29\n",
      "Epoch: [153] \t Loss 0.2313 \t Acc 47.25 \t AccHead 51.64 \t AccTail 42.86\n",
      "Epoch: [154] \t Loss 0.1933 \t Acc 45.68 \t AccHead 51.97 \t AccTail 39.39\n",
      "Epoch: [155] \t Loss 0.1793 \t Acc 47.83 \t AccHead 53.24 \t AccTail 42.42\n",
      "Epoch: [156] \t Loss 0.1404 \t Acc 48.58 \t AccHead 53.44 \t AccTail 43.72\n",
      "Epoch: [157] \t Loss 0.1268 \t Acc 49.17 \t AccHead 53.11 \t AccTail 45.24\n",
      "Epoch: [158] \t Loss 0.1098 \t Acc 48.63 \t AccHead 53.31 \t AccTail 43.94\n",
      "Epoch: [159] \t Loss 0.1053 \t Acc 48.78 \t AccHead 54.05 \t AccTail 43.51\n",
      "Epoch: [160] \t Loss 0.1022 \t Acc 47.30 \t AccHead 53.91 \t AccTail 40.69\n",
      "Epoch: [161] \t Loss 0.0861 \t Acc 46.90 \t AccHead 53.11 \t AccTail 40.69\n",
      "Epoch: [162] \t Loss 0.0936 \t Acc 47.41 \t AccHead 53.04 \t AccTail 41.77\n",
      "Epoch: [163] \t Loss 0.0897 \t Acc 48.83 \t AccHead 53.51 \t AccTail 44.16\n",
      "Epoch: [164] \t Loss 0.0693 \t Acc 48.53 \t AccHead 53.98 \t AccTail 43.07\n",
      "Epoch: [165] \t Loss 0.0747 \t Acc 48.89 \t AccHead 54.72 \t AccTail 43.07\n",
      "Epoch: [166] \t Loss 0.0686 \t Acc 47.99 \t AccHead 53.98 \t AccTail 41.99\n",
      "Epoch: [167] \t Loss 0.0731 \t Acc 47.68 \t AccHead 53.58 \t AccTail 41.77\n",
      "Epoch: [168] \t Loss 0.0667 \t Acc 47.78 \t AccHead 53.78 \t AccTail 41.77\n",
      "Epoch: [169] \t Loss 0.0507 \t Acc 48.35 \t AccHead 53.85 \t AccTail 42.86\n",
      "Epoch: [170] \t Loss 0.0643 \t Acc 48.79 \t AccHead 54.52 \t AccTail 43.07\n",
      "Epoch: [171] \t Loss 0.0576 \t Acc 48.70 \t AccHead 54.98 \t AccTail 42.42\n",
      "Epoch: [172] \t Loss 0.0488 \t Acc 48.27 \t AccHead 54.11 \t AccTail 42.42\n",
      "Epoch: [173] \t Loss 0.0505 \t Acc 48.66 \t AccHead 54.25 \t AccTail 43.07\n",
      "Epoch: [174] \t Loss 0.0472 \t Acc 47.83 \t AccHead 54.31 \t AccTail 41.34\n",
      "Epoch: [175] \t Loss 0.0414 \t Acc 48.91 \t AccHead 54.31 \t AccTail 43.51\n",
      "Epoch: [176] \t Loss 0.0442 \t Acc 49.41 \t AccHead 55.32 \t AccTail 43.51\n",
      "Epoch: [177] \t Loss 0.0478 \t Acc 48.14 \t AccHead 53.85 \t AccTail 42.42\n",
      "Epoch: [178] \t Loss 0.0449 \t Acc 49.46 \t AccHead 54.98 \t AccTail 43.94\n",
      "Epoch: [179] \t Loss 0.0517 \t Acc 48.79 \t AccHead 54.72 \t AccTail 42.86\n",
      "Epoch: [180] \t Loss 0.0479 \t Acc 50.21 \t AccHead 54.31 \t AccTail 46.10\n",
      "Epoch: [181] \t Loss 0.0479 \t Acc 48.29 \t AccHead 54.38 \t AccTail 42.21\n",
      "Epoch: [182] \t Loss 0.0459 \t Acc 49.20 \t AccHead 54.25 \t AccTail 44.16\n",
      "Epoch: [183] \t Loss 0.0386 \t Acc 48.14 \t AccHead 54.52 \t AccTail 41.77\n",
      "Epoch: [184] \t Loss 0.0366 \t Acc 49.50 \t AccHead 53.98 \t AccTail 45.02\n",
      "Epoch: [185] \t Loss 0.0446 \t Acc 49.76 \t AccHead 54.72 \t AccTail 44.81\n",
      "Epoch: [186] \t Loss 0.0398 \t Acc 49.41 \t AccHead 55.32 \t AccTail 43.51\n",
      "Epoch: [187] \t Loss 0.0354 \t Acc 48.58 \t AccHead 54.52 \t AccTail 42.64\n",
      "Epoch: [188] \t Loss 0.0333 \t Acc 48.33 \t AccHead 54.45 \t AccTail 42.21\n",
      "Epoch: [189] \t Loss 0.0367 \t Acc 48.92 \t AccHead 54.98 \t AccTail 42.86\n",
      "Epoch: [190] \t Loss 0.0355 \t Acc 49.83 \t AccHead 55.72 \t AccTail 43.94\n",
      "Epoch: [191] \t Loss 0.0349 \t Acc 49.39 \t AccHead 54.85 \t AccTail 43.94\n",
      "Epoch: [192] \t Loss 0.0343 \t Acc 48.97 \t AccHead 54.65 \t AccTail 43.29\n",
      "Epoch: [193] \t Loss 0.0354 \t Acc 49.43 \t AccHead 54.92 \t AccTail 43.94\n",
      "Epoch: [194] \t Loss 0.0360 \t Acc 50.53 \t AccHead 55.38 \t AccTail 45.67\n",
      "Epoch: [195] \t Loss 0.0316 \t Acc 49.61 \t AccHead 55.72 \t AccTail 43.51\n",
      "Epoch: [196] \t Loss 0.0321 \t Acc 48.64 \t AccHead 54.65 \t AccTail 42.64\n",
      "Epoch: [197] \t Loss 0.0347 \t Acc 49.10 \t AccHead 54.92 \t AccTail 43.29\n",
      "Epoch: [198] \t Loss 0.0320 \t Acc 49.07 \t AccHead 53.98 \t AccTail 44.16\n",
      "Epoch: [199] \t Loss 0.0280 \t Acc 49.33 \t AccHead 54.72 \t AccTail 43.94\n",
      "Epoch: [200] \t Loss 0.0328 \t Acc 49.02 \t AccHead 54.31 \t AccTail 43.72\n"
     ]
    }
   ],
   "source": [
    "print('STEP 7: TRAIN THE MODEL')\n",
    "cri=num_classes//2-1\n",
    "max_norm = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_history = []\n",
    "    # train\n",
    "    model_head.train()\n",
    "    model_tail.train()\n",
    "    for batch_index, data in enumerate(train_loader):\n",
    "        image, target = data\n",
    "        image, target = image.cuda(), target.cuda()\n",
    "        \n",
    "        image_tail = image[target>cri]\n",
    "        target_tail = target[target>cri]\n",
    "        image_head = image[target<=cri]\n",
    "        target_head = target[target<=cri]\n",
    "            \n",
    "        pred_head = model_head(image_head)\n",
    "        loss_head = criterion(pred_head, target_head)\n",
    "        optimizer_head.zero_grad()\n",
    "        loss_head.backward()\n",
    "        optimizer_head.step()\n",
    "        \n",
    "        if target_tail.size()[0] == 1 :\n",
    "            image_tail=torch.cat([image_tail, image_tail])\n",
    "            target_tail = torch.cat([target_tail, target_tail])\n",
    "            \n",
    "        if target_tail.size()[0] == 0 :\n",
    "            loss = loss_head\n",
    "#             print('ok')\n",
    "        else :\n",
    "            pred_tail = model_tail(image_tail)\n",
    "            loss_tail = criterion(pred_tail, target_tail)\n",
    "            optimizer_tail.zero_grad()\n",
    "            optimizer_tail.zero_grad()\n",
    "            loss_tail.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model_tail.parameters(), max_norm)\n",
    "            optimizer_tail.step()     \n",
    "            loss = loss_head+loss_tail\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "        \n",
    "    # eval\n",
    "    topk_acc, head_acc_ok, tail_acc = compute_accuracy(val_loader, model_head)\n",
    "    topk_acc, head_acc, tail_acc_ok = compute_accuracy(val_loader, model_tail)\n",
    "    \n",
    "    topk_acc = (head_acc_ok[0]+ tail_acc_ok[0])/2\n",
    "    tail_acc = tail_acc_ok[0]\n",
    "    head_acc = head_acc_ok[0]\n",
    "    \n",
    "    loss_mean = np.mean(loss_history)\n",
    "    scheduler_head.step()\n",
    "    scheduler_tail.step()\n",
    "\n",
    "    print('Epoch: [{:03d}] \\t Loss {:.4f} \\t Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(epoch+1, loss_mean, topk_acc, head_acc, tail_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51dea1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_head': model_head.state_dict(),\n",
    "    'optimizer_head': optimizer_head.state_dict(),\n",
    "    'model_tail': model_tail.state_dict(),\n",
    "    'optimizer_tail': optimizer_tail.state_dict(),\n",
    "    'epoch': epoch},\n",
    "    osp.join(SAVE_DIR, 'ep{:03d}.pth'.format(epoch+1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "673b80a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 82.06 \t AccHead 83.66 \t AccTail 80.46\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = 'logs/final/CIFAR10/exp-0.1' \n",
    "model_head = ResNet18(num_classes)\n",
    "model_tail = ResNet18(num_classes)\n",
    "model_head.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model_head'])\n",
    "model_tail.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model_tail'])\n",
    "model_head = model_head.cuda()\n",
    "model_tail = model_tail.cuda()\n",
    "\n",
    "topk_acc, head_acc_ok, tail_acc = compute_accuracy(test_loader, model_head)\n",
    "topk_acc, head_acc, tail_acc_ok = compute_accuracy(test_loader, model_tail)\n",
    "topk_acc = (head_acc_ok[0]+ tail_acc_ok[0])/2\n",
    "tail_acc = tail_acc_ok[0]\n",
    "head_acc = head_acc_ok[0]\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc, head_acc, tail_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a5fdd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 61.10 \t AccHead 75.32 \t AccTail 46.88\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = 'logs/final/CIFAR10/exp-0.01' \n",
    "model_head = ResNet18(num_classes)\n",
    "model_tail = ResNet18(num_classes)\n",
    "model_head.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model_head'])\n",
    "model_tail.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model_tail'])\n",
    "model_head = model_head.cuda()\n",
    "model_tail = model_tail.cuda()\n",
    "\n",
    "topk_acc, head_acc_ok, tail_acc = compute_accuracy(test_loader, model_head)\n",
    "topk_acc, head_acc, tail_acc_ok = compute_accuracy(test_loader, model_tail)\n",
    "topk_acc = (head_acc_ok[0]+ tail_acc_ok[0])/2\n",
    "tail_acc = tail_acc_ok[0]\n",
    "head_acc = head_acc_ok[0]\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc, head_acc, tail_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "118daa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 79.66 \t AccHead 84.58 \t AccTail 74.74\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = 'logs/final/CIFAR10/step-0.1' \n",
    "model_head = ResNet18(num_classes)\n",
    "model_tail = ResNet18(num_classes)\n",
    "model_head.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model_head'])\n",
    "model_tail.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model_tail'])\n",
    "model_head = model_head.cuda()\n",
    "model_tail = model_tail.cuda()\n",
    "\n",
    "topk_acc, head_acc_ok, tail_acc = compute_accuracy(test_loader, model_head)\n",
    "topk_acc, head_acc, tail_acc_ok = compute_accuracy(test_loader, model_tail)\n",
    "topk_acc = (head_acc_ok[0]+ tail_acc_ok[0])/2\n",
    "tail_acc = tail_acc_ok[0]\n",
    "head_acc = head_acc_ok[0]\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc, head_acc, tail_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43533099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 50.53 \t AccHead 86.80 \t AccTail 14.26\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = 'logs/final/CIFAR10/step-0.01' \n",
    "model_head = ResNet18(num_classes)\n",
    "model_tail = ResNet18(num_classes)\n",
    "model_head.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model_head'])\n",
    "model_tail.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model_tail'])\n",
    "model_head = model_head.cuda()\n",
    "model_tail = model_tail.cuda()\n",
    "\n",
    "topk_acc, head_acc_ok, tail_acc = compute_accuracy(test_loader, model_head)\n",
    "topk_acc, head_acc, tail_acc_ok = compute_accuracy(test_loader, model_tail)\n",
    "topk_acc = (head_acc_ok[0]+ tail_acc_ok[0])/2\n",
    "tail_acc = tail_acc_ok[0]\n",
    "head_acc = head_acc_ok[0]\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc, head_acc, tail_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e9b7a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 41.39 \t AccHead 47.48 \t AccTail 35.30\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = 'logs/final/CIFAR100/exp-0.1' \n",
    "model_head = ResNet18(num_classes)\n",
    "model_tail = ResNet18(num_classes)\n",
    "model_head.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model_head'])\n",
    "model_tail.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model_tail'])\n",
    "model_head = model_head.cuda()\n",
    "model_tail = model_tail.cuda()\n",
    "\n",
    "topk_acc, head_acc_ok, tail_acc = compute_accuracy(test_loader, model_head)\n",
    "topk_acc, head_acc, tail_acc_ok = compute_accuracy(test_loader, model_tail)\n",
    "topk_acc = (head_acc_ok[0]+ tail_acc_ok[0])/2\n",
    "tail_acc = tail_acc_ok[0]\n",
    "head_acc = head_acc_ok[0]\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc, head_acc, tail_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c6b5906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 28.07 \t AccHead 40.16 \t AccTail 15.98\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = 'logs/final/CIFAR100/exp-0.01' \n",
    "model_head = ResNet18(num_classes)\n",
    "model_tail = ResNet18(num_classes)\n",
    "model_head.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model_head'])\n",
    "model_tail.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model_tail'])\n",
    "model_head = model_head.cuda()\n",
    "model_tail = model_tail.cuda()\n",
    "\n",
    "topk_acc, head_acc_ok, tail_acc = compute_accuracy(test_loader, model_head)\n",
    "topk_acc, head_acc, tail_acc_ok = compute_accuracy(test_loader, model_tail)\n",
    "topk_acc = (head_acc_ok[0]+ tail_acc_ok[0])/2\n",
    "tail_acc = tail_acc_ok[0]\n",
    "head_acc = head_acc_ok[0]\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc, head_acc, tail_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89a84091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 39.09 \t AccHead 54.28 \t AccTail 23.90\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = 'logs/final/CIFAR100/step-0.1' \n",
    "model_head = ResNet18(num_classes)\n",
    "model_tail = ResNet18(num_classes)\n",
    "model_head.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model_head'])\n",
    "model_tail.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model_tail'])\n",
    "model_head = model_head.cuda()\n",
    "model_tail = model_tail.cuda()\n",
    "\n",
    "topk_acc, head_acc_ok, tail_acc = compute_accuracy(test_loader, model_head)\n",
    "topk_acc, head_acc, tail_acc_ok = compute_accuracy(test_loader, model_tail)\n",
    "topk_acc = (head_acc_ok[0]+ tail_acc_ok[0])/2\n",
    "tail_acc = tail_acc_ok[0]\n",
    "head_acc = head_acc_ok[0]\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc, head_acc, tail_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7228972e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 27.62 \t AccHead 54.10 \t AccTail 1.14\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = 'logs/final/CIFAR100/step-0.01' \n",
    "model_head = ResNet18(num_classes)\n",
    "model_tail = ResNet18(num_classes)\n",
    "model_head.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model_head'])\n",
    "model_tail.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model_tail'])\n",
    "model_head = model_head.cuda()\n",
    "model_tail = model_tail.cuda()\n",
    "\n",
    "topk_acc, head_acc_ok, tail_acc = compute_accuracy(test_loader, model_head)\n",
    "topk_acc, head_acc, tail_acc_ok = compute_accuracy(test_loader, model_tail)\n",
    "topk_acc = (head_acc_ok[0]+ tail_acc_ok[0])/2\n",
    "tail_acc = tail_acc_ok[0]\n",
    "head_acc = head_acc_ok[0]\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc, head_acc, tail_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2f8bbf",
   "metadata": {},
   "source": [
    "이건 CIFAR100에서 경향성을 알기 위해 epoch200까지 돌린 결과이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95e1f1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 48.56 \t AccHead 54.34 \t AccTail 42.78\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = 'logs/final/CIFAR100/exp-0.1' \n",
    "model_head = ResNet18(num_classes)\n",
    "model_tail = ResNet18(num_classes)\n",
    "model_head.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep200.pth')\n",
    ")['model_head'])\n",
    "model_tail.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep200.pth')\n",
    ")['model_tail'])\n",
    "model_head = model_head.cuda()\n",
    "model_tail = model_tail.cuda()\n",
    "\n",
    "topk_acc, head_acc_ok, tail_acc = compute_accuracy(test_loader, model_head)\n",
    "topk_acc, head_acc, tail_acc_ok = compute_accuracy(test_loader, model_tail)\n",
    "topk_acc = (head_acc_ok[0]+ tail_acc_ok[0])/2\n",
    "tail_acc = tail_acc_ok[0]\n",
    "head_acc = head_acc_ok[0]\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc, head_acc, tail_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4d475c",
   "metadata": {},
   "source": [
    "실행 시간을 위한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab26f387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 7: TRAIN THE MODEL\n",
      "Epoch: [001] \t Loss 4.1206 \t Acc 33.18 \t AccHead 38.05 \t AccTail 28.30\n",
      "Epoch: [002] \t Loss 2.7381 \t Acc 48.15 \t AccHead 52.44 \t AccTail 43.87\n",
      "Epoch: [003] \t Loss 2.4852 \t Acc 52.66 \t AccHead 57.44 \t AccTail 47.88\n",
      "Epoch: [004] \t Loss 2.2800 \t Acc 54.37 \t AccHead 58.74 \t AccTail 50.00\n",
      "Epoch: [005] \t Loss 2.1457 \t Acc 60.75 \t AccHead 63.00 \t AccTail 58.49\n",
      "Epoch: [006] \t Loss 1.9617 \t Acc 62.36 \t AccHead 62.45 \t AccTail 62.26\n",
      "Epoch: [007] \t Loss 1.9203 \t Acc 59.98 \t AccHead 59.11 \t AccTail 60.85\n",
      "Epoch: [008] \t Loss 1.7997 \t Acc 59.59 \t AccHead 65.16 \t AccTail 54.01\n",
      "Epoch: [009] \t Loss 1.7629 \t Acc 68.16 \t AccHead 70.29 \t AccTail 66.04\n",
      "Epoch: [010] \t Loss 1.6852 \t Acc 70.46 \t AccHead 71.59 \t AccTail 69.34\n",
      "Epoch: [011] \t Loss 1.6154 \t Acc 71.19 \t AccHead 70.91 \t AccTail 71.46\n",
      "Epoch: [012] \t Loss 1.5355 \t Acc 70.19 \t AccHead 73.63 \t AccTail 66.75\n",
      "Epoch: [013] \t Loss 1.5088 \t Acc 70.86 \t AccHead 74.74 \t AccTail 66.98\n",
      "Epoch: [014] \t Loss 1.4578 \t Acc 72.87 \t AccHead 73.56 \t AccTail 72.17\n",
      "Epoch: [015] \t Loss 1.3878 \t Acc 71.24 \t AccHead 75.73 \t AccTail 66.75\n",
      "Epoch: [016] \t Loss 1.3635 \t Acc 75.10 \t AccHead 77.08 \t AccTail 73.11\n",
      "Epoch: [017] \t Loss 1.3071 \t Acc 73.00 \t AccHead 76.90 \t AccTail 69.10\n",
      "Epoch: [018] \t Loss 1.2947 \t Acc 73.32 \t AccHead 76.59 \t AccTail 70.05\n",
      "Epoch: [019] \t Loss 1.2586 \t Acc 76.12 \t AccHead 76.53 \t AccTail 75.71\n",
      "Epoch: [020] \t Loss 1.2192 \t Acc 76.08 \t AccHead 78.57 \t AccTail 73.58\n",
      "Epoch: [021] \t Loss 1.2113 \t Acc 75.29 \t AccHead 79.12 \t AccTail 71.46\n",
      "Epoch: [022] \t Loss 1.1763 \t Acc 77.67 \t AccHead 78.69 \t AccTail 76.65\n",
      "Epoch: [023] \t Loss 1.1458 \t Acc 75.47 \t AccHead 77.83 \t AccTail 73.11\n",
      "Epoch: [024] \t Loss 1.1302 \t Acc 78.95 \t AccHead 79.12 \t AccTail 78.77\n",
      "Epoch: [025] \t Loss 1.0973 \t Acc 76.98 \t AccHead 79.43 \t AccTail 74.53\n",
      "Epoch: [026] \t Loss 1.0803 \t Acc 77.75 \t AccHead 80.98 \t AccTail 74.53\n",
      "Epoch: [027] \t Loss 1.0686 \t Acc 77.52 \t AccHead 79.80 \t AccTail 75.24\n",
      "Epoch: [028] \t Loss 1.0716 \t Acc 77.00 \t AccHead 80.42 \t AccTail 73.58\n",
      "Epoch: [029] \t Loss 1.0121 \t Acc 76.83 \t AccHead 78.20 \t AccTail 75.47\n",
      "Epoch: [030] \t Loss 0.9982 \t Acc 78.59 \t AccHead 81.47 \t AccTail 75.71\n",
      "Epoch: [031] \t Loss 0.9882 \t Acc 79.16 \t AccHead 81.90 \t AccTail 76.42\n",
      "Epoch: [032] \t Loss 0.9821 \t Acc 75.99 \t AccHead 81.22 \t AccTail 70.75\n",
      "Epoch: [033] \t Loss 0.9739 \t Acc 78.28 \t AccHead 79.43 \t AccTail 77.12\n",
      "Epoch: [034] \t Loss 0.9833 \t Acc 80.67 \t AccHead 83.76 \t AccTail 77.59\n",
      "Epoch: [035] \t Loss 0.9347 \t Acc 79.65 \t AccHead 82.89 \t AccTail 76.42\n",
      "Epoch: [036] \t Loss 0.9355 \t Acc 79.76 \t AccHead 83.82 \t AccTail 75.71\n",
      "Epoch: [037] \t Loss 0.9017 \t Acc 79.54 \t AccHead 81.72 \t AccTail 77.36\n",
      "Epoch: [038] \t Loss 0.8858 \t Acc 80.48 \t AccHead 84.06 \t AccTail 76.89\n",
      "Epoch: [039] \t Loss 0.8856 \t Acc 76.34 \t AccHead 83.57 \t AccTail 69.10\n",
      "Epoch: [040] \t Loss 0.8758 \t Acc 80.66 \t AccHead 83.01 \t AccTail 78.30\n",
      "Epoch: [041] \t Loss 0.8840 \t Acc 77.12 \t AccHead 81.59 \t AccTail 72.64\n",
      "Epoch: [042] \t Loss 0.8767 \t Acc 81.26 \t AccHead 83.76 \t AccTail 78.77\n",
      "Epoch: [043] \t Loss 0.8691 \t Acc 82.17 \t AccHead 84.62 \t AccTail 79.72\n",
      "Epoch: [044] \t Loss 0.8469 \t Acc 81.77 \t AccHead 84.06 \t AccTail 79.48\n",
      "Epoch: [045] \t Loss 0.8438 \t Acc 80.39 \t AccHead 82.95 \t AccTail 77.83\n",
      "Epoch: [046] \t Loss 0.8001 \t Acc 83.38 \t AccHead 83.51 \t AccTail 83.25\n",
      "Epoch: [047] \t Loss 0.7975 \t Acc 82.43 \t AccHead 83.26 \t AccTail 81.60\n",
      "Epoch: [048] \t Loss 0.8149 \t Acc 79.95 \t AccHead 84.43 \t AccTail 75.47\n",
      "Epoch: [049] \t Loss 0.7834 \t Acc 80.75 \t AccHead 82.03 \t AccTail 79.48\n",
      "Epoch: [050] \t Loss 0.8196 \t Acc 81.58 \t AccHead 83.45 \t AccTail 79.72\n",
      "Epoch: [051] \t Loss 0.7726 \t Acc 82.56 \t AccHead 84.93 \t AccTail 80.19\n",
      "Epoch: [052] \t Loss 0.7547 \t Acc 80.34 \t AccHead 82.15 \t AccTail 78.54\n",
      "Epoch: [053] \t Loss 0.7687 \t Acc 83.81 \t AccHead 84.13 \t AccTail 83.49\n",
      "Epoch: [054] \t Loss 0.7491 \t Acc 82.70 \t AccHead 84.74 \t AccTail 80.66\n",
      "Epoch: [055] \t Loss 0.7390 \t Acc 80.32 \t AccHead 84.93 \t AccTail 75.71\n",
      "Epoch: [056] \t Loss 0.7423 \t Acc 81.51 \t AccHead 82.83 \t AccTail 80.19\n",
      "Epoch: [057] \t Loss 0.7319 \t Acc 82.61 \t AccHead 84.81 \t AccTail 80.42\n",
      "Epoch: [058] \t Loss 0.7250 \t Acc 83.48 \t AccHead 85.11 \t AccTail 81.84\n",
      "Epoch: [059] \t Loss 0.7216 \t Acc 80.32 \t AccHead 83.76 \t AccTail 76.89\n",
      "Epoch: [060] \t Loss 0.6836 \t Acc 82.51 \t AccHead 83.88 \t AccTail 81.13\n",
      "Epoch: [061] \t Loss 0.7075 \t Acc 81.77 \t AccHead 83.82 \t AccTail 79.72\n",
      "Epoch: [062] \t Loss 0.6981 \t Acc 82.21 \t AccHead 84.93 \t AccTail 79.48\n",
      "Epoch: [063] \t Loss 0.6946 \t Acc 81.42 \t AccHead 84.31 \t AccTail 78.54\n",
      "Epoch: [064] \t Loss 0.7083 \t Acc 82.52 \t AccHead 83.20 \t AccTail 81.84\n",
      "Epoch: [065] \t Loss 0.6950 \t Acc 84.21 \t AccHead 84.93 \t AccTail 83.49\n",
      "Epoch: [066] \t Loss 0.6533 \t Acc 83.30 \t AccHead 84.99 \t AccTail 81.60\n",
      "Epoch: [067] \t Loss 0.6816 \t Acc 83.11 \t AccHead 83.45 \t AccTail 82.78\n",
      "Epoch: [068] \t Loss 0.6941 \t Acc 79.75 \t AccHead 82.15 \t AccTail 77.36\n",
      "Epoch: [069] \t Loss 0.7051 \t Acc 83.14 \t AccHead 84.43 \t AccTail 81.84\n",
      "Epoch: [070] \t Loss 0.6581 \t Acc 83.68 \t AccHead 85.05 \t AccTail 82.31\n",
      "Epoch: [071] \t Loss 0.6602 \t Acc 83.17 \t AccHead 84.74 \t AccTail 81.60\n",
      "Epoch: [072] \t Loss 0.6447 \t Acc 83.84 \t AccHead 84.43 \t AccTail 83.25\n",
      "Epoch: [073] \t Loss 0.6276 \t Acc 83.39 \t AccHead 86.60 \t AccTail 80.19\n",
      "Epoch: [074] \t Loss 0.6329 \t Acc 81.88 \t AccHead 83.82 \t AccTail 79.95\n",
      "Epoch: [075] \t Loss 0.6675 \t Acc 83.29 \t AccHead 84.50 \t AccTail 82.08\n",
      "Epoch: [076] \t Loss 0.6374 \t Acc 84.11 \t AccHead 85.92 \t AccTail 82.31\n",
      "Epoch: [077] \t Loss 0.6304 \t Acc 83.97 \t AccHead 85.86 \t AccTail 82.08\n",
      "Epoch: [078] \t Loss 0.6156 \t Acc 83.25 \t AccHead 85.61 \t AccTail 80.90\n",
      "Epoch: [079] \t Loss 0.6669 \t Acc 78.64 \t AccHead 84.87 \t AccTail 72.41\n",
      "Epoch: [080] \t Loss 0.6809 \t Acc 83.60 \t AccHead 85.36 \t AccTail 81.84\n",
      "Epoch: [081] \t Loss 0.6089 \t Acc 83.51 \t AccHead 85.42 \t AccTail 81.60\n",
      "Epoch: [082] \t Loss 0.6144 \t Acc 82.53 \t AccHead 85.11 \t AccTail 79.95\n",
      "Epoch: [083] \t Loss 0.6252 \t Acc 83.90 \t AccHead 86.66 \t AccTail 81.13\n",
      "Epoch: [084] \t Loss 0.6316 \t Acc 81.83 \t AccHead 86.78 \t AccTail 76.89\n",
      "Epoch: [085] \t Loss 0.6141 \t Acc 81.74 \t AccHead 82.58 \t AccTail 80.90\n",
      "Epoch: [086] \t Loss 0.5944 \t Acc 81.87 \t AccHead 83.32 \t AccTail 80.42\n",
      "Epoch: [087] \t Loss 0.5993 \t Acc 83.47 \t AccHead 84.87 \t AccTail 82.08\n",
      "Epoch: [088] \t Loss 0.6135 \t Acc 84.40 \t AccHead 86.72 \t AccTail 82.08\n",
      "Epoch: [089] \t Loss 0.6128 \t Acc 83.40 \t AccHead 85.42 \t AccTail 81.37\n",
      "Epoch: [090] \t Loss 0.5855 \t Acc 84.17 \t AccHead 85.79 \t AccTail 82.55\n",
      "Epoch: [091] \t Loss 0.5814 \t Acc 82.42 \t AccHead 85.11 \t AccTail 79.72\n",
      "Epoch: [092] \t Loss 0.5927 \t Acc 83.65 \t AccHead 84.99 \t AccTail 82.31\n",
      "Epoch: [093] \t Loss 0.5628 \t Acc 82.78 \t AccHead 85.36 \t AccTail 80.19\n",
      "Epoch: [094] \t Loss 0.5791 \t Acc 84.24 \t AccHead 84.99 \t AccTail 83.49\n",
      "Epoch: [095] \t Loss 0.6086 \t Acc 83.19 \t AccHead 85.48 \t AccTail 80.90\n",
      "Epoch: [096] \t Loss 0.5826 \t Acc 84.05 \t AccHead 83.20 \t AccTail 84.91\n",
      "Epoch: [097] \t Loss 0.5507 \t Acc 83.01 \t AccHead 84.19 \t AccTail 81.84\n",
      "Epoch: [098] \t Loss 0.5402 \t Acc 85.84 \t AccHead 85.11 \t AccTail 86.56\n",
      "Epoch: [099] \t Loss 0.5542 \t Acc 82.15 \t AccHead 85.05 \t AccTail 79.25\n",
      "Epoch: [100] \t Loss 0.5728 \t Acc 82.00 \t AccHead 83.57 \t AccTail 80.42\n",
      "1869.8253557682037\n"
     ]
    }
   ],
   "source": [
    "print('STEP 7: TRAIN THE MODEL')\n",
    "cri=num_classes//2-1\n",
    "max_norm = 5\n",
    "\n",
    "import time \n",
    "\n",
    "train_start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_history = []\n",
    "    # train\n",
    "    model_head.train()\n",
    "    model_tail.train()\n",
    "    for batch_index, data in enumerate(train_loader):\n",
    "        image, target = data\n",
    "        image, target = image.cuda(), target.cuda()\n",
    "        \n",
    "        image_tail = image[target>cri]\n",
    "        target_tail = target[target>cri]\n",
    "        image_head = image[target<=cri]\n",
    "        target_head = target[target<=cri]\n",
    "            \n",
    "        pred_head = model_head(image_head)\n",
    "        loss_head = criterion(pred_head, target_head)\n",
    "        optimizer_head.zero_grad()\n",
    "        loss_head.backward()\n",
    "        optimizer_head.step()\n",
    "        \n",
    "        if target_tail.size()[0] == 1 :\n",
    "            image_tail=torch.cat([image_tail, image_tail])\n",
    "            target_tail = torch.cat([target_tail, target_tail])\n",
    "            \n",
    "        if target_tail.size()[0] == 0 :\n",
    "            loss = loss_head\n",
    "#             print('ok')\n",
    "        else :\n",
    "            pred_tail = model_tail(image_tail)\n",
    "            loss_tail = criterion(pred_tail, target_tail)\n",
    "            optimizer_tail.zero_grad()\n",
    "            optimizer_tail.zero_grad()\n",
    "            loss_tail.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model_tail.parameters(), max_norm)\n",
    "            optimizer_tail.step()     \n",
    "            loss = loss_head+loss_tail\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "        \n",
    "    # eval\n",
    "    topk_acc, head_acc_ok, tail_acc = compute_accuracy(val_loader, model_head)\n",
    "    topk_acc, head_acc, tail_acc_ok = compute_accuracy(val_loader, model_tail)\n",
    "    \n",
    "    topk_acc = (head_acc_ok[0]+ tail_acc_ok[0])/2\n",
    "    tail_acc = tail_acc_ok[0]\n",
    "    head_acc = head_acc_ok[0]\n",
    "    \n",
    "    loss_mean = np.mean(loss_history)\n",
    "    scheduler_head.step()\n",
    "    scheduler_tail.step()\n",
    "\n",
    "    print('Epoch: [{:03d}] \\t Loss {:.4f} \\t Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(epoch+1, loss_mean, topk_acc, head_acc, tail_acc))\n",
    "\n",
    "train_end_time = time.time()\n",
    "\n",
    "print(train_end_time-train_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7b47d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 81.12 \t AccHead 80.38 \t AccTail 81.86\n",
      "10.042887687683105\n"
     ]
    }
   ],
   "source": [
    "train_start_time = time.time()\n",
    "\n",
    "topk_acc, head_acc_ok, tail_acc = compute_accuracy(test_loader, model_head)\n",
    "topk_acc, head_acc, tail_acc_ok = compute_accuracy(test_loader, model_tail)\n",
    "topk_acc = (head_acc_ok[0]+ tail_acc_ok[0])/2\n",
    "tail_acc = tail_acc_ok[0]\n",
    "head_acc = head_acc_ok[0]\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc, head_acc, tail_acc))\n",
    "\n",
    "train_end_time = time.time()\n",
    "\n",
    "print(train_end_time-train_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f154a857",
   "metadata": {},
   "source": [
    "여기서는 결과를 살펴볼 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "82177c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "example = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "76eeb675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_image = example[0][0]\n",
    "example_label = example[1][0]\n",
    "example_image = (example_image+1)/2\n",
    "example_image = example_image.cpu().data.numpy()\n",
    "example_image = np.transpose(example_image, [1,2,0])\n",
    "plt.imshow(example_image)\n",
    "print(example_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5d20717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image = example[0][0]\n",
    "data_test=example_image.unsqueeze(0)\n",
    "data_test = data_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fa977108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -3.3564,  -2.9364,  -4.9331,   3.3350,  -2.6883, -10.8467, -10.8931,\n",
      "         -10.8949, -10.8388, -10.8795]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[-10.9268, -10.9313, -10.9291, -10.9262, -10.9306,   0.8470,  -2.0368,\n",
      "          -2.9228,  -7.0908,  -6.8276]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred_head = model_head(data_test)\n",
    "pred_tail = model_tail(data_test)\n",
    "print(pred_head)\n",
    "print(pred_tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d96cc540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.3564, -2.9364, -4.9331,  3.3350, -2.6883,  0.8470, -2.0368, -2.9228,\n",
      "        -7.0908, -6.8276], device='cuda:0', grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred = torch.cat([pred_head[0][:5],pred_tail[0][5:]])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4fcdb3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3, device='cuda:0')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fcabd823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_image = example[0][1]\n",
    "example_label = example[1][1]\n",
    "example_image = (example_image+1)/2\n",
    "example_image = example_image.cpu().data.numpy()\n",
    "example_image = np.transpose(example_image, [1,2,0])\n",
    "plt.imshow(example_image)\n",
    "print(example_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "399a6724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.8470,  0.3829, -3.0873, -3.9216, -4.5899, -6.9575, -6.9526, -6.9562,\n",
      "         -6.9638, -6.9557]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([[-10.7455, -10.7424, -10.7524, -10.7529, -10.7584,  -2.7019,  -4.3051,\n",
      "          -2.4733,   4.3881,  -1.0011]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([ 1.8470,  0.3829, -3.0873, -3.9216, -4.5899, -2.7019, -4.3051, -2.4733,\n",
      "         4.3881, -1.0011], device='cuda:0', grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(8, device='cuda:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_image = example[0][1]\n",
    "data_test=example_image.unsqueeze(0)\n",
    "data_test = data_test.cuda()\n",
    "pred_head = model_head(data_test)\n",
    "pred_tail = model_tail(data_test)\n",
    "print(pred_head)\n",
    "print(pred_tail)\n",
    "pred = torch.cat([pred_head[0][:5],pred_tail[0][5:]])\n",
    "print(pred)\n",
    "torch.argmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f52a3bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_image = example[0][99]\n",
    "example_label = example[1][99]\n",
    "example_image = (example_image+1)/2\n",
    "example_image = example_image.cpu().data.numpy()\n",
    "example_image = np.transpose(example_image, [1,2,0])\n",
    "plt.imshow(example_image)\n",
    "print(example_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4e8f4af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.0348, -4.8249, -3.2829,  2.5847, -1.0996, -9.5114, -9.5376, -9.5386,\n",
      "         -9.5094, -9.5313]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([[-6.3900, -6.3927, -6.3896, -6.3891, -6.3916, -0.1007, -1.4665,  0.5020,\n",
      "         -4.7876, -2.5988]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([-4.0348, -4.8249, -3.2829,  2.5847, -1.0996, -0.1007, -1.4665,  0.5020,\n",
      "        -4.7876, -2.5988], device='cuda:0', grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3, device='cuda:0')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_image = example[0][99]\n",
    "data_test=example_image.unsqueeze(0)\n",
    "data_test = data_test.cuda()\n",
    "pred_head = model_head(data_test)\n",
    "pred_tail = model_tail(data_test)\n",
    "print(pred_head)\n",
    "print(pred_tail)\n",
    "pred = torch.cat([pred_head[0][:5],pred_tail[0][5:]])\n",
    "print(pred)\n",
    "torch.argmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2ce7ba89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0636, -0.0760, -0.0517,  0.0407, -0.0173, -0.1499, -0.1503, -0.1503,\n",
      "         -0.1499, -0.1502]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[-0.1543, -0.1544, -0.1543, -0.1543, -0.1544, -0.0024, -0.0354,  0.0121,\n",
      "         -0.1156, -0.0628]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([-0.0636, -0.0760, -0.0517,  0.0407, -0.0173, -0.0024, -0.0354,  0.0121,\n",
      "        -0.1156, -0.0628], device='cuda:0', grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3, device='cuda:0')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_image = example[0][99]\n",
    "data_test=example_image.unsqueeze(0)\n",
    "data_test = data_test.cuda()\n",
    "pred_head = model_head(data_test)\n",
    "pred_head_abs = torch.abs(pred_head)\n",
    "pred_head_sum = torch.sum(pred_head_abs)\n",
    "pred_head /= pred_head_sum\n",
    "pred_tail = model_tail(data_test)\n",
    "pred_tail_abs = torch.abs(pred_tail)\n",
    "pred_tail_sum = torch.sum(pred_tail_abs)\n",
    "pred_tail /= pred_tail_sum\n",
    "print(pred_head)\n",
    "print(pred_tail)\n",
    "pred = torch.cat([pred_head[0][:5],pred_tail[0][5:]])\n",
    "print(pred)\n",
    "torch.argmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49151da0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
