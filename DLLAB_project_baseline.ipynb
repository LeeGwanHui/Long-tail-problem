{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be564ffe-7c6a-4983-a927-c53f79fe530b",
   "metadata": {},
   "source": [
    "> ### EEE4423: Deep Learning Lab\n",
    "\n",
    "# Final Project: Long-tail Visual Recognition for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d8d4f-d166-4203-9223-f5b675b2ebd8",
   "metadata": {},
   "source": [
    "<h4><div style=\"text-align: right\"> Due date: June 24, 2022.  </div> <br>\n",
    "<div style=\"text-align: right\"> Please upload your file @ LearnUs and submit via e-mail by 2 PM in the form of [ID_Name_project.ipynb]. </div></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0d7d3b2-32a4-4808-9476-edc89d829705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from misc.project.utils import resnet18, IMBALANCECIFAR10, IMBALANCECIFAR100, compute_accuracy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) ## 경고 무시\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03866650-32bc-4868-bcd7-eca0888adda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6: INSTANTIATE OPTIMIZER CLASS\n"
     ]
    }
   ],
   "source": [
    "print('STEP 6: INSTANTIATE OPTIMIZER CLASS')\n",
    "\n",
    "DATASET = 'CIFAR10' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'exp' #['exp', 'step'] # 질문\n",
    "IMB_FACTOR = 0.1 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/baseline/CIFAR10/exp-0.1' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128 \n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 2e-4\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebfa934f-3869-4519-86e1-abd9173b62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "665d1d9f-ea6a-4d00-bd1e-e42d28565546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: LOADING DATASET\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[5000, 3871, 2997, 2320, 1796, 1391, 1077, 834, 645, 500]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print('STEP 1: LOADING DATASET')\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "if DATASET == 'CIFAR10':\n",
    "    train_dataset_0 = IMBALANCECIFAR10(root='../dataset/project', imb_type=IMB_TYPE, imb_factor=IMB_FACTOR, train=True, download=True, transform=transform_train)\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root='../dataset/project', train=False, download=True, transform=transform_test)\n",
    "elif DATASET == 'CIFAR100':\n",
    "    train_dataset_0 = IMBALANCECIFAR100(root='../dataset/project', imb_type=IMB_TYPE, imb_factor=IMB_FACTOR, train=True, download=True, transform=transform_train)\n",
    "    test_dataset = torchvision.datasets.CIFAR100(root='../dataset/project', train=False, download=True, transform=transform_test)\n",
    "\n",
    "cls_num_list = train_dataset_0.get_cls_num_list() # 각 class에 몇개가 들어있는지 나타냄 \n",
    "print('cls num list:')\n",
    "print(cls_num_list)\n",
    "num_classes = len(cls_num_list)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c76d1276-055c-4c8a-bfdd-5e2d2fb4ebad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20431\n",
      "18388\n",
      "2043\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "torch.manual_seed(torch.initial_seed())\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(0)\n",
    "dataset_size = len(train_dataset_0)\n",
    "validation_size = int(dataset_size * 0.1) if not int(dataset_size * 0.1)==0 else 1 \n",
    "train_size = dataset_size - validation_size\n",
    "test_size = 0\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset_0 = random_split(train_dataset_0, [train_size, validation_size, test_size],generator=generator)\n",
    "\n",
    "print(len(train_dataset_0))\n",
    "print(len(train_dataset))\n",
    "print(len(validation_dataset))\n",
    "print(len(test_dataset_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "550108fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: MAKING DATASET ITERABLE\n"
     ]
    }
   ],
   "source": [
    "print('STEP 2: MAKING DATASET ITERABLE')\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=4, drop_last=False)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    validation_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=4, drop_last=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=100, shuffle=False,\n",
    "    num_workers=4, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b1a3fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4500. 3446. 2684. 2104. 1631. 1254.  974.  757.  582.  456.]\n",
      "18388.0\n"
     ]
    }
   ],
   "source": [
    "# 다른 방법이긴하지만 이렇게 표현가능  \n",
    "list1 = np.zeros(10)\n",
    "for batch_index, data in enumerate(train_loader):\n",
    "    image, target = data\n",
    "    for j in range(len(target)) :\n",
    "        k =target[j] \n",
    "        list1[k]+=1\n",
    "\n",
    "print(list1)\n",
    "print(list1.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1a22399-4ce8-4320-ab5c-813fdfa045d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: CREATE MODEL CLASS (VGG16)\n"
     ]
    }
   ],
   "source": [
    "print('STEP 3: CREATE MODEL CLASS (VGG16)')\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.backbone = resnet18()\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone(x)\n",
    "        x = F.adaptive_max_pool2d(x, 1)\n",
    "        x = x.view(batch_size, -1)\n",
    "        pred = self.classifier(x)\n",
    "#         print(pred.size()) # torch.Size([128, 10])\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afdb726e-f03a-4032-904f-deb21613086a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: INSTANTIATE MODEL CLASS\n",
      "STEP 5: INSTANTIATE LOSS CLASS\n"
     ]
    }
   ],
   "source": [
    "print('STEP 4: INSTANTIATE MODEL CLASS')\n",
    "\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "\n",
    "print('STEP 5: INSTANTIATE LOSS CLASS')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc43023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea710170-226d-4093-84e0-cacbee35ed81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [001] \t Loss 4.4817 \t Acc 9.71 \t AccHead 12.11 \t AccTail 1.95\n",
      "Epoch: [002] \t Loss 3.7678 \t Acc 14.61 \t AccHead 17.99 \t AccTail 3.68\n",
      "Epoch: [003] \t Loss 3.5381 \t Acc 16.71 \t AccHead 20.33 \t AccTail 4.98\n",
      "Epoch: [004] \t Loss 3.3571 \t Acc 18.91 \t AccHead 21.67 \t AccTail 9.96\n",
      "Epoch: [005] \t Loss 3.1880 \t Acc 22.99 \t AccHead 26.89 \t AccTail 10.39\n",
      "Epoch: [006] \t Loss 3.0758 \t Acc 22.23 \t AccHead 25.42 \t AccTail 11.90\n",
      "Epoch: [007] \t Loss 2.9630 \t Acc 24.43 \t AccHead 27.69 \t AccTail 13.85\n",
      "Epoch: [008] \t Loss 2.8484 \t Acc 25.75 \t AccHead 29.90 \t AccTail 12.34\n",
      "Epoch: [009] \t Loss 2.7459 \t Acc 28.67 \t AccHead 32.24 \t AccTail 17.10\n",
      "Epoch: [010] \t Loss 2.6549 \t Acc 28.87 \t AccHead 32.91 \t AccTail 15.80\n",
      "Epoch: [011] \t Loss 2.5433 \t Acc 29.84 \t AccHead 33.98 \t AccTail 16.45\n",
      "Epoch: [012] \t Loss 2.4764 \t Acc 31.37 \t AccHead 35.32 \t AccTail 18.61\n",
      "Epoch: [013] \t Loss 2.3736 \t Acc 31.53 \t AccHead 35.65 \t AccTail 18.18\n",
      "Epoch: [014] \t Loss 2.2983 \t Acc 32.60 \t AccHead 35.92 \t AccTail 21.86\n",
      "Epoch: [015] \t Loss 2.2485 \t Acc 33.52 \t AccHead 37.39 \t AccTail 21.00\n",
      "Epoch: [016] \t Loss 2.1730 \t Acc 33.27 \t AccHead 37.06 \t AccTail 21.00\n",
      "Epoch: [017] \t Loss 2.0972 \t Acc 35.21 \t AccHead 39.06 \t AccTail 22.73\n",
      "Epoch: [018] \t Loss 2.0038 \t Acc 35.97 \t AccHead 39.93 \t AccTail 23.16\n",
      "Epoch: [019] \t Loss 1.9499 \t Acc 35.41 \t AccHead 39.93 \t AccTail 20.78\n",
      "Epoch: [020] \t Loss 1.8597 \t Acc 35.56 \t AccHead 39.60 \t AccTail 22.51\n",
      "Epoch: [021] \t Loss 1.8223 \t Acc 36.69 \t AccHead 40.94 \t AccTail 22.94\n",
      "Epoch: [022] \t Loss 1.7651 \t Acc 37.00 \t AccHead 41.54 \t AccTail 22.29\n",
      "Epoch: [023] \t Loss 1.7047 \t Acc 37.86 \t AccHead 41.20 \t AccTail 27.06\n",
      "Epoch: [024] \t Loss 1.6349 \t Acc 36.08 \t AccHead 40.00 \t AccTail 23.38\n",
      "Epoch: [025] \t Loss 1.5912 \t Acc 35.62 \t AccHead 39.26 \t AccTail 23.81\n",
      "Epoch: [026] \t Loss 1.5247 \t Acc 37.00 \t AccHead 40.94 \t AccTail 24.24\n",
      "Epoch: [027] \t Loss 1.4661 \t Acc 37.61 \t AccHead 41.61 \t AccTail 24.68\n",
      "Epoch: [028] \t Loss 1.4366 \t Acc 39.24 \t AccHead 43.21 \t AccTail 26.41\n",
      "Epoch: [029] \t Loss 1.3713 \t Acc 38.22 \t AccHead 41.14 \t AccTail 28.79\n",
      "Epoch: [030] \t Loss 1.3186 \t Acc 38.43 \t AccHead 42.54 \t AccTail 25.11\n",
      "Epoch: [031] \t Loss 1.2777 \t Acc 38.63 \t AccHead 42.47 \t AccTail 26.19\n",
      "Epoch: [032] \t Loss 1.2367 \t Acc 37.81 \t AccHead 41.40 \t AccTail 26.19\n",
      "Epoch: [033] \t Loss 1.1704 \t Acc 37.86 \t AccHead 42.41 \t AccTail 23.16\n",
      "Epoch: [034] \t Loss 1.1370 \t Acc 37.10 \t AccHead 40.40 \t AccTail 26.41\n",
      "Epoch: [035] \t Loss 1.0799 \t Acc 39.14 \t AccHead 43.68 \t AccTail 24.46\n",
      "Epoch: [036] \t Loss 1.0154 \t Acc 37.56 \t AccHead 40.33 \t AccTail 28.57\n",
      "Epoch: [037] \t Loss 0.9833 \t Acc 38.89 \t AccHead 42.61 \t AccTail 26.84\n",
      "Epoch: [038] \t Loss 0.9655 \t Acc 37.92 \t AccHead 41.27 \t AccTail 27.06\n",
      "Epoch: [039] \t Loss 0.9254 \t Acc 37.35 \t AccHead 40.67 \t AccTail 26.62\n",
      "Epoch: [040] \t Loss 0.8805 \t Acc 36.13 \t AccHead 39.20 \t AccTail 26.19\n",
      "Epoch: [041] \t Loss 0.8675 \t Acc 38.83 \t AccHead 41.61 \t AccTail 29.87\n",
      "Epoch: [042] \t Loss 0.8112 \t Acc 39.09 \t AccHead 42.47 \t AccTail 28.14\n",
      "Epoch: [043] \t Loss 0.7606 \t Acc 38.27 \t AccHead 41.94 \t AccTail 26.41\n",
      "Epoch: [044] \t Loss 0.7559 \t Acc 37.05 \t AccHead 40.54 \t AccTail 25.76\n",
      "Epoch: [045] \t Loss 0.7455 \t Acc 38.12 \t AccHead 41.61 \t AccTail 26.84\n",
      "Epoch: [046] \t Loss 0.6914 \t Acc 39.70 \t AccHead 43.21 \t AccTail 28.35\n",
      "Epoch: [047] \t Loss 0.6579 \t Acc 38.73 \t AccHead 41.61 \t AccTail 29.44\n",
      "Epoch: [048] \t Loss 0.6483 \t Acc 39.70 \t AccHead 43.01 \t AccTail 29.00\n",
      "Epoch: [049] \t Loss 0.6487 \t Acc 37.61 \t AccHead 40.60 \t AccTail 27.92\n",
      "Epoch: [050] \t Loss 0.6150 \t Acc 38.63 \t AccHead 42.07 \t AccTail 27.49\n",
      "Epoch: [051] \t Loss 0.6026 \t Acc 39.40 \t AccHead 43.34 \t AccTail 26.62\n",
      "Epoch: [052] \t Loss 0.5837 \t Acc 38.94 \t AccHead 43.08 \t AccTail 25.54\n",
      "Epoch: [053] \t Loss 0.5596 \t Acc 39.35 \t AccHead 43.01 \t AccTail 27.49\n",
      "Epoch: [054] \t Loss 0.5410 \t Acc 38.38 \t AccHead 42.14 \t AccTail 26.19\n",
      "Epoch: [055] \t Loss 0.5358 \t Acc 38.89 \t AccHead 43.55 \t AccTail 23.81\n",
      "Epoch: [056] \t Loss 0.4768 \t Acc 37.86 \t AccHead 41.27 \t AccTail 26.84\n",
      "Epoch: [057] \t Loss 0.4922 \t Acc 37.76 \t AccHead 41.00 \t AccTail 27.27\n",
      "Epoch: [058] \t Loss 0.4855 \t Acc 38.53 \t AccHead 42.41 \t AccTail 25.97\n",
      "Epoch: [059] \t Loss 0.4667 \t Acc 38.02 \t AccHead 41.54 \t AccTail 26.62\n",
      "Epoch: [060] \t Loss 0.4500 \t Acc 38.48 \t AccHead 42.07 \t AccTail 26.84\n",
      "Epoch: [061] \t Loss 0.4565 \t Acc 39.96 \t AccHead 43.81 \t AccTail 27.49\n",
      "Epoch: [062] \t Loss 0.4443 \t Acc 38.02 \t AccHead 41.40 \t AccTail 27.06\n",
      "Epoch: [063] \t Loss 0.4851 \t Acc 37.20 \t AccHead 40.40 \t AccTail 26.84\n",
      "Epoch: [064] \t Loss 0.4264 \t Acc 37.05 \t AccHead 39.93 \t AccTail 27.71\n",
      "Epoch: [065] \t Loss 0.4359 \t Acc 37.86 \t AccHead 41.47 \t AccTail 26.19\n",
      "Epoch: [066] \t Loss 0.4431 \t Acc 38.48 \t AccHead 42.01 \t AccTail 27.06\n",
      "Epoch: [067] \t Loss 0.4116 \t Acc 37.46 \t AccHead 41.34 \t AccTail 24.89\n",
      "Epoch: [068] \t Loss 0.4025 \t Acc 38.48 \t AccHead 42.27 \t AccTail 26.19\n",
      "Epoch: [069] \t Loss 0.4020 \t Acc 38.83 \t AccHead 42.21 \t AccTail 27.92\n",
      "Epoch: [070] \t Loss 0.4174 \t Acc 40.21 \t AccHead 44.35 \t AccTail 26.84\n",
      "Epoch: [071] \t Loss 0.3835 \t Acc 36.74 \t AccHead 40.33 \t AccTail 25.11\n",
      "Epoch: [072] \t Loss 0.3696 \t Acc 38.58 \t AccHead 42.07 \t AccTail 27.27\n",
      "Epoch: [073] \t Loss 0.3898 \t Acc 38.17 \t AccHead 42.54 \t AccTail 24.03\n",
      "Epoch: [074] \t Loss 0.4073 \t Acc 39.29 \t AccHead 41.81 \t AccTail 31.17\n",
      "Epoch: [075] \t Loss 0.3556 \t Acc 39.14 \t AccHead 43.01 \t AccTail 26.62\n",
      "Epoch: [076] \t Loss 0.3388 \t Acc 39.40 \t AccHead 41.94 \t AccTail 31.17\n",
      "Epoch: [077] \t Loss 0.3844 \t Acc 38.02 \t AccHead 41.94 \t AccTail 25.32\n",
      "Epoch: [078] \t Loss 0.3850 \t Acc 37.10 \t AccHead 40.00 \t AccTail 27.71\n",
      "Epoch: [079] \t Loss 0.3746 \t Acc 39.65 \t AccHead 43.75 \t AccTail 26.41\n",
      "Epoch: [080] \t Loss 0.3580 \t Acc 38.48 \t AccHead 42.81 \t AccTail 24.46\n",
      "Epoch: [081] \t Loss 0.3397 \t Acc 38.94 \t AccHead 43.14 \t AccTail 25.32\n",
      "Epoch: [082] \t Loss 0.3568 \t Acc 39.65 \t AccHead 43.55 \t AccTail 27.06\n",
      "Epoch: [083] \t Loss 0.3615 \t Acc 37.35 \t AccHead 41.20 \t AccTail 24.89\n",
      "Epoch: [084] \t Loss 0.3484 \t Acc 37.61 \t AccHead 41.47 \t AccTail 25.11\n",
      "Epoch: [085] \t Loss 0.3537 \t Acc 39.35 \t AccHead 43.01 \t AccTail 27.49\n",
      "Epoch: [086] \t Loss 0.3438 \t Acc 38.99 \t AccHead 42.21 \t AccTail 28.57\n",
      "Epoch: [087] \t Loss 0.3470 \t Acc 37.15 \t AccHead 41.07 \t AccTail 24.46\n",
      "Epoch: [088] \t Loss 0.3786 \t Acc 39.04 \t AccHead 43.61 \t AccTail 24.24\n",
      "Epoch: [089] \t Loss 0.3496 \t Acc 39.75 \t AccHead 43.41 \t AccTail 27.92\n",
      "Epoch: [090] \t Loss 0.3400 \t Acc 38.63 \t AccHead 43.08 \t AccTail 24.24\n",
      "Epoch: [091] \t Loss 0.3511 \t Acc 38.78 \t AccHead 42.01 \t AccTail 28.35\n",
      "Epoch: [092] \t Loss 0.3079 \t Acc 37.81 \t AccHead 41.67 \t AccTail 25.32\n",
      "Epoch: [093] \t Loss 0.3460 \t Acc 39.29 \t AccHead 43.28 \t AccTail 26.41\n",
      "Epoch: [094] \t Loss 0.3455 \t Acc 38.63 \t AccHead 42.54 \t AccTail 25.97\n",
      "Epoch: [095] \t Loss 0.3111 \t Acc 39.29 \t AccHead 42.61 \t AccTail 28.57\n",
      "Epoch: [096] \t Loss 0.3280 \t Acc 37.61 \t AccHead 40.94 \t AccTail 26.84\n",
      "Epoch: [097] \t Loss 0.3586 \t Acc 38.63 \t AccHead 42.41 \t AccTail 26.41\n",
      "Epoch: [098] \t Loss 0.3206 \t Acc 39.29 \t AccHead 43.01 \t AccTail 27.27\n",
      "Epoch: [099] \t Loss 0.3072 \t Acc 39.14 \t AccHead 42.21 \t AccTail 29.22\n",
      "Epoch: [100] \t Loss 0.3040 \t Acc 38.99 \t AccHead 42.21 \t AccTail 28.57\n",
      "Epoch: [101] \t Loss 0.3247 \t Acc 40.11 \t AccHead 43.48 \t AccTail 29.22\n",
      "Epoch: [102] \t Loss 0.2947 \t Acc 38.12 \t AccHead 42.07 \t AccTail 25.32\n",
      "Epoch: [103] \t Loss 0.3320 \t Acc 38.94 \t AccHead 42.14 \t AccTail 28.57\n",
      "Epoch: [104] \t Loss 0.3354 \t Acc 38.22 \t AccHead 41.47 \t AccTail 27.71\n",
      "Epoch: [105] \t Loss 0.3479 \t Acc 37.86 \t AccHead 41.34 \t AccTail 26.62\n",
      "Epoch: [106] \t Loss 0.3379 \t Acc 39.29 \t AccHead 41.81 \t AccTail 31.17\n",
      "Epoch: [107] \t Loss 0.2842 \t Acc 39.60 \t AccHead 43.81 \t AccTail 25.97\n",
      "Epoch: [108] \t Loss 0.2895 \t Acc 38.53 \t AccHead 42.14 \t AccTail 26.84\n",
      "Epoch: [109] \t Loss 0.3122 \t Acc 39.45 \t AccHead 43.08 \t AccTail 27.71\n",
      "Epoch: [110] \t Loss 0.2998 \t Acc 40.47 \t AccHead 44.21 \t AccTail 28.35\n",
      "Epoch: [111] \t Loss 0.3041 \t Acc 39.40 \t AccHead 42.61 \t AccTail 29.00\n",
      "Epoch: [112] \t Loss 0.3021 \t Acc 38.27 \t AccHead 41.81 \t AccTail 26.84\n",
      "Epoch: [113] \t Loss 0.3162 \t Acc 38.32 \t AccHead 41.00 \t AccTail 29.65\n",
      "Epoch: [114] \t Loss 0.2943 \t Acc 39.29 \t AccHead 42.94 \t AccTail 27.49\n",
      "Epoch: [115] \t Loss 0.2972 \t Acc 38.78 \t AccHead 42.94 \t AccTail 25.32\n",
      "Epoch: [116] \t Loss 0.2989 \t Acc 39.96 \t AccHead 43.81 \t AccTail 27.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [117] \t Loss 0.3242 \t Acc 39.04 \t AccHead 43.28 \t AccTail 25.32\n",
      "Epoch: [118] \t Loss 0.2863 \t Acc 38.48 \t AccHead 41.61 \t AccTail 28.35\n",
      "Epoch: [119] \t Loss 0.3084 \t Acc 39.91 \t AccHead 43.81 \t AccTail 27.27\n",
      "Epoch: [120] \t Loss 0.2876 \t Acc 37.76 \t AccHead 42.01 \t AccTail 24.03\n",
      "Epoch: [121] \t Loss 0.2990 \t Acc 37.86 \t AccHead 41.27 \t AccTail 26.84\n",
      "Epoch: [122] \t Loss 0.3101 \t Acc 38.32 \t AccHead 41.81 \t AccTail 27.06\n",
      "Epoch: [123] \t Loss 0.3095 \t Acc 38.17 \t AccHead 41.14 \t AccTail 28.57\n",
      "Epoch: [124] \t Loss 0.2772 \t Acc 41.29 \t AccHead 45.02 \t AccTail 29.22\n",
      "Epoch: [125] \t Loss 0.3008 \t Acc 39.35 \t AccHead 44.28 \t AccTail 23.38\n",
      "Epoch: [126] \t Loss 0.3133 \t Acc 38.22 \t AccHead 41.74 \t AccTail 26.84\n",
      "Epoch: [127] \t Loss 0.3262 \t Acc 40.21 \t AccHead 45.35 \t AccTail 23.59\n",
      "Epoch: [128] \t Loss 0.2939 \t Acc 40.16 \t AccHead 44.62 \t AccTail 25.76\n",
      "Epoch: [129] \t Loss 0.2993 \t Acc 38.22 \t AccHead 42.07 \t AccTail 25.76\n",
      "Epoch: [130] \t Loss 0.3005 \t Acc 37.56 \t AccHead 40.54 \t AccTail 27.92\n",
      "Epoch: [131] \t Loss 0.2859 \t Acc 39.40 \t AccHead 42.68 \t AccTail 28.79\n",
      "Epoch: [132] \t Loss 0.2574 \t Acc 38.27 \t AccHead 42.07 \t AccTail 25.97\n",
      "Epoch: [133] \t Loss 0.2828 \t Acc 39.09 \t AccHead 42.94 \t AccTail 26.62\n",
      "Epoch: [134] \t Loss 0.2722 \t Acc 39.96 \t AccHead 44.01 \t AccTail 26.84\n",
      "Epoch: [135] \t Loss 0.2994 \t Acc 38.32 \t AccHead 42.54 \t AccTail 24.68\n",
      "Epoch: [136] \t Loss 0.2872 \t Acc 37.00 \t AccHead 40.40 \t AccTail 25.97\n",
      "Epoch: [137] \t Loss 0.2670 \t Acc 38.53 \t AccHead 42.27 \t AccTail 26.41\n",
      "Epoch: [138] \t Loss 0.2863 \t Acc 38.12 \t AccHead 41.94 \t AccTail 25.76\n",
      "Epoch: [139] \t Loss 0.3064 \t Acc 38.53 \t AccHead 42.34 \t AccTail 26.19\n",
      "Epoch: [140] \t Loss 0.3112 \t Acc 39.55 \t AccHead 43.48 \t AccTail 26.84\n",
      "Epoch: [141] \t Loss 0.2814 \t Acc 39.70 \t AccHead 43.41 \t AccTail 27.71\n",
      "Epoch: [142] \t Loss 0.2935 \t Acc 38.78 \t AccHead 41.87 \t AccTail 28.79\n",
      "Epoch: [143] \t Loss 0.2866 \t Acc 38.07 \t AccHead 41.87 \t AccTail 25.76\n",
      "Epoch: [144] \t Loss 0.2745 \t Acc 38.58 \t AccHead 42.88 \t AccTail 24.68\n",
      "Epoch: [145] \t Loss 0.2653 \t Acc 37.51 \t AccHead 40.87 \t AccTail 26.62\n",
      "Epoch: [146] \t Loss 0.2770 \t Acc 40.16 \t AccHead 43.28 \t AccTail 30.09\n",
      "Epoch: [147] \t Loss 0.2635 \t Acc 38.94 \t AccHead 43.55 \t AccTail 24.03\n",
      "Epoch: [148] \t Loss 0.2992 \t Acc 37.40 \t AccHead 41.34 \t AccTail 24.68\n",
      "Epoch: [149] \t Loss 0.2874 \t Acc 39.14 \t AccHead 43.95 \t AccTail 23.59\n",
      "Epoch: [150] \t Loss 0.2833 \t Acc 38.94 \t AccHead 42.54 \t AccTail 27.27\n",
      "Epoch: [151] \t Loss 0.1095 \t Acc 43.69 \t AccHead 47.76 \t AccTail 30.52\n",
      "Epoch: [152] \t Loss 0.0487 \t Acc 44.66 \t AccHead 49.36 \t AccTail 29.44\n",
      "Epoch: [153] \t Loss 0.0386 \t Acc 45.32 \t AccHead 50.70 \t AccTail 27.92\n",
      "Epoch: [154] \t Loss 0.0290 \t Acc 45.02 \t AccHead 49.57 \t AccTail 30.30\n",
      "Epoch: [155] \t Loss 0.0274 \t Acc 45.63 \t AccHead 50.23 \t AccTail 30.74\n",
      "Epoch: [156] \t Loss 0.0221 \t Acc 44.30 \t AccHead 49.16 \t AccTail 28.57\n",
      "Epoch: [157] \t Loss 0.0191 \t Acc 43.89 \t AccHead 48.76 \t AccTail 28.14\n",
      "Epoch: [158] \t Loss 0.0183 \t Acc 44.92 \t AccHead 50.23 \t AccTail 27.71\n",
      "Epoch: [159] \t Loss 0.0160 \t Acc 45.43 \t AccHead 50.37 \t AccTail 29.44\n",
      "Epoch: [160] \t Loss 0.0156 \t Acc 45.48 \t AccHead 50.03 \t AccTail 30.74\n",
      "Epoch: [161] \t Loss 0.0159 \t Acc 44.10 \t AccHead 48.76 \t AccTail 29.00\n",
      "Epoch: [162] \t Loss 0.0143 \t Acc 45.27 \t AccHead 50.43 \t AccTail 28.57\n",
      "Epoch: [163] \t Loss 0.0129 \t Acc 45.58 \t AccHead 50.50 \t AccTail 29.65\n",
      "Epoch: [164] \t Loss 0.0129 \t Acc 45.89 \t AccHead 51.04 \t AccTail 29.22\n",
      "Epoch: [165] \t Loss 0.0121 \t Acc 45.43 \t AccHead 50.03 \t AccTail 30.52\n",
      "Epoch: [166] \t Loss 0.0117 \t Acc 44.92 \t AccHead 49.63 \t AccTail 29.65\n",
      "Epoch: [167] \t Loss 0.0109 \t Acc 45.17 \t AccHead 50.03 \t AccTail 29.44\n",
      "Epoch: [168] \t Loss 0.0107 \t Acc 45.99 \t AccHead 49.77 \t AccTail 33.77\n",
      "Epoch: [169] \t Loss 0.0111 \t Acc 44.97 \t AccHead 49.70 \t AccTail 29.65\n",
      "Epoch: [170] \t Loss 0.0103 \t Acc 45.99 \t AccHead 51.10 \t AccTail 29.44\n",
      "Epoch: [171] \t Loss 0.0096 \t Acc 45.43 \t AccHead 49.97 \t AccTail 30.74\n",
      "Epoch: [172] \t Loss 0.0099 \t Acc 45.53 \t AccHead 50.17 \t AccTail 30.52\n",
      "Epoch: [173] \t Loss 0.0096 \t Acc 45.02 \t AccHead 49.36 \t AccTail 30.95\n",
      "Epoch: [174] \t Loss 0.0091 \t Acc 46.04 \t AccHead 50.70 \t AccTail 30.95\n",
      "Epoch: [175] \t Loss 0.0092 \t Acc 45.38 \t AccHead 50.10 \t AccTail 30.09\n",
      "Epoch: [176] \t Loss 0.0092 \t Acc 46.30 \t AccHead 50.77 \t AccTail 31.82\n",
      "Epoch: [177] \t Loss 0.0088 \t Acc 44.51 \t AccHead 49.16 \t AccTail 29.44\n",
      "Epoch: [178] \t Loss 0.0078 \t Acc 45.63 \t AccHead 49.70 \t AccTail 32.47\n",
      "Epoch: [179] \t Loss 0.0083 \t Acc 45.78 \t AccHead 49.97 \t AccTail 32.25\n",
      "Epoch: [180] \t Loss 0.0079 \t Acc 45.78 \t AccHead 50.50 \t AccTail 30.52\n",
      "Epoch: [181] \t Loss 0.0081 \t Acc 45.53 \t AccHead 50.10 \t AccTail 30.74\n",
      "Epoch: [182] \t Loss 0.0076 \t Acc 45.27 \t AccHead 49.70 \t AccTail 30.95\n",
      "Epoch: [183] \t Loss 0.0081 \t Acc 45.73 \t AccHead 49.77 \t AccTail 32.68\n",
      "Epoch: [184] \t Loss 0.0075 \t Acc 45.38 \t AccHead 49.90 \t AccTail 30.74\n",
      "Epoch: [185] \t Loss 0.0075 \t Acc 46.91 \t AccHead 51.64 \t AccTail 31.60\n",
      "Epoch: [186] \t Loss 0.0072 \t Acc 45.84 \t AccHead 49.83 \t AccTail 32.90\n",
      "Epoch: [187] \t Loss 0.0072 \t Acc 46.55 \t AccHead 50.77 \t AccTail 32.90\n",
      "Epoch: [188] \t Loss 0.0069 \t Acc 45.78 \t AccHead 50.43 \t AccTail 30.74\n",
      "Epoch: [189] \t Loss 0.0068 \t Acc 46.35 \t AccHead 50.57 \t AccTail 32.68\n",
      "Epoch: [190] \t Loss 0.0069 \t Acc 46.55 \t AccHead 51.24 \t AccTail 31.39\n",
      "Epoch: [191] \t Loss 0.0072 \t Acc 46.19 \t AccHead 50.70 \t AccTail 31.60\n",
      "Epoch: [192] \t Loss 0.0070 \t Acc 46.35 \t AccHead 51.04 \t AccTail 31.17\n",
      "Epoch: [193] \t Loss 0.0070 \t Acc 45.27 \t AccHead 49.83 \t AccTail 30.52\n",
      "Epoch: [194] \t Loss 0.0070 \t Acc 46.76 \t AccHead 51.30 \t AccTail 32.03\n",
      "Epoch: [195] \t Loss 0.0068 \t Acc 45.32 \t AccHead 50.17 \t AccTail 29.65\n",
      "Epoch: [196] \t Loss 0.0067 \t Acc 45.22 \t AccHead 49.83 \t AccTail 30.30\n",
      "Epoch: [197] \t Loss 0.0062 \t Acc 46.30 \t AccHead 50.77 \t AccTail 31.82\n",
      "Epoch: [198] \t Loss 0.0063 \t Acc 47.11 \t AccHead 51.71 \t AccTail 32.25\n",
      "Epoch: [199] \t Loss 0.0064 \t Acc 45.99 \t AccHead 50.70 \t AccTail 30.74\n",
      "Epoch: [200] \t Loss 0.0062 \t Acc 45.94 \t AccHead 50.77 \t AccTail 30.30\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    loss_history = []\n",
    "    model.train()\n",
    "    for batch_index, data in enumerate(train_loader):\n",
    "        image, target = data\n",
    "        image, target = image.cuda(), target.cuda()\n",
    "\n",
    "        pred = model(image)\n",
    "        loss = criterion(pred, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "    topk_acc, head_acc, tail_acc = compute_accuracy(val_loader, model)\n",
    "    loss_mean = np.mean(loss_history)\n",
    "    scheduler.step()\n",
    "\n",
    "    print('Epoch: [{:03d}] \\t Loss {:.4f} \\t Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(epoch+1, loss_mean, topk_acc[0], head_acc[0], tail_acc[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89632525",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epoch': epoch},\n",
    "    osp.join(SAVE_DIR, 'ep{:03d}.pth'.format(epoch+1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a4789ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 70.78 \t AccHead 76.90 \t AccTail 64.66\n"
     ]
    }
   ],
   "source": [
    "##  CIFAR 10 : exp : 0.1\n",
    "SAVE_DIR = 'logs/baseline/CIFAR10/exp-0.1'\n",
    "model = ResNet18(num_classes)\n",
    "model.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model'])\n",
    "model = model.cuda()\n",
    "topk_acc, head_acc, tail_acc = compute_accuracy(test_loader, model)\n",
    "\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc[0], head_acc[0], tail_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "928a5e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 51.47 \t AccHead 71.48 \t AccTail 31.46\n"
     ]
    }
   ],
   "source": [
    "##  CIFAR 10 : exp : 0.01 \n",
    "SAVE_DIR = 'logs/baseline/CIFAR10/exp-0.01' \n",
    "model = ResNet18(num_classes)\n",
    "model.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model'])\n",
    "model = model.cuda()\n",
    "topk_acc, head_acc, tail_acc = compute_accuracy(test_loader, model)\n",
    "\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc[0], head_acc[0], tail_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "893ef33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 69.09 \t AccHead 79.76 \t AccTail 58.42\n"
     ]
    }
   ],
   "source": [
    "## CIFAR 10 : step : 0.1\n",
    "SAVE_DIR = 'logs/baseline/CIFAR10/step-0.1' \n",
    "model = ResNet18(num_classes)\n",
    "model.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model'])\n",
    "model = model.cuda()\n",
    "topk_acc, head_acc, tail_acc = compute_accuracy(test_loader, model)\n",
    "\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc[0], head_acc[0], tail_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "517a7f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 48.26 \t AccHead 85.94 \t AccTail 10.58\n"
     ]
    }
   ],
   "source": [
    "## CIFAR 10 : step : 0.01\n",
    "SAVE_DIR = 'logs/baseline/CIFAR10/step-0.01' \n",
    "model = ResNet18(num_classes)\n",
    "model.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model'])\n",
    "model = model.cuda()\n",
    "topk_acc, head_acc, tail_acc = compute_accuracy(test_loader, model)\n",
    "\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc[0], head_acc[0], tail_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a00828f",
   "metadata": {},
   "source": [
    "여기서 부터는 CIFAR 100 임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2eee9376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 33.60 \t AccHead 42.48 \t AccTail 24.72\n"
     ]
    }
   ],
   "source": [
    "## CIFAR 100 : exp : 0.1 -> 위에 부터 바꾸고 다시 시작해야함.\n",
    "SAVE_DIR = 'logs/baseline/CIFAR100/exp-0.1'\n",
    "model = ResNet18(num_classes)\n",
    "model.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model'])\n",
    "model = model.cuda()\n",
    "topk_acc, head_acc, tail_acc = compute_accuracy(test_loader, model)\n",
    "\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc[0], head_acc[0], tail_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77adb72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 24.69 \t AccHead 38.68 \t AccTail 10.70\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "SAVE_DIR = 'logs/baseline/CIFAR100/exp-0.01'\n",
    "model = ResNet18(num_classes)\n",
    "model.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model'])\n",
    "model = model.cuda()\n",
    "topk_acc, head_acc, tail_acc = compute_accuracy(test_loader, model)\n",
    "\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc[0], head_acc[0], tail_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5250ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 34.35 \t AccHead 50.80 \t AccTail 17.90\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "SAVE_DIR = 'logs/baseline/CIFAR100/step-0.1'\n",
    "model = ResNet18(num_classes)\n",
    "model.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model'])\n",
    "model = model.cuda()\n",
    "topk_acc, head_acc, tail_acc = compute_accuracy(test_loader, model)\n",
    "\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc[0], head_acc[0], tail_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8755f217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 27.10 \t AccHead 53.22 \t AccTail 0.98\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "SAVE_DIR = 'logs/baseline/CIFAR100/step-0.01'\n",
    "model = ResNet18(num_classes)\n",
    "model.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep100.pth')\n",
    ")['model'])\n",
    "model = model.cuda()\n",
    "topk_acc, head_acc, tail_acc = compute_accuracy(test_loader, model)\n",
    "\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc[0], head_acc[0], tail_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40939e63",
   "metadata": {},
   "source": [
    "이건 CIFAR100에서 경향성을 알기 위해 epoch200까지 돌린 결과이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a499388",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 39.99 \t AccHead 50.64 \t AccTail 29.34\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "SAVE_DIR = 'logs/baseline/CIFAR100/exp-0.1'\n",
    "model = ResNet18(num_classes)\n",
    "model.load_state_dict(torch.load(\n",
    "    osp.join(SAVE_DIR, 'ep200.pth')\n",
    ")['model'])\n",
    "model = model.cuda()\n",
    "topk_acc, head_acc, tail_acc = compute_accuracy(test_loader, model)\n",
    "\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc[0], head_acc[0], tail_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b0de57",
   "metadata": {},
   "source": [
    "실행 시간을 위한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28e5506e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [001] \t Loss 2.5427 \t Acc 38.96 \t AccHead 49.17 \t AccTail 0.00\n",
      "Epoch: [002] \t Loss 1.6652 \t Acc 45.96 \t AccHead 57.75 \t AccTail 0.94\n",
      "Epoch: [003] \t Loss 1.5391 \t Acc 47.04 \t AccHead 55.34 \t AccTail 15.33\n",
      "Epoch: [004] \t Loss 1.4422 \t Acc 53.21 \t AccHead 62.01 \t AccTail 19.58\n",
      "Epoch: [005] \t Loss 1.3590 \t Acc 52.96 \t AccHead 61.33 \t AccTail 20.99\n",
      "Epoch: [006] \t Loss 1.3086 \t Acc 54.82 \t AccHead 63.62 \t AccTail 21.23\n",
      "Epoch: [007] \t Loss 1.2369 \t Acc 57.66 \t AccHead 64.67 \t AccTail 30.90\n",
      "Epoch: [008] \t Loss 1.2075 \t Acc 59.47 \t AccHead 65.23 \t AccTail 37.50\n",
      "Epoch: [009] \t Loss 1.1447 \t Acc 58.69 \t AccHead 65.23 \t AccTail 33.73\n",
      "Epoch: [010] \t Loss 1.1038 \t Acc 61.63 \t AccHead 69.18 \t AccTail 32.78\n",
      "Epoch: [011] \t Loss 1.0532 \t Acc 62.95 \t AccHead 68.93 \t AccTail 40.09\n",
      "Epoch: [012] \t Loss 1.0076 \t Acc 66.13 \t AccHead 72.58 \t AccTail 41.51\n",
      "Epoch: [013] \t Loss 0.9664 \t Acc 63.97 \t AccHead 70.91 \t AccTail 37.50\n",
      "Epoch: [014] \t Loss 0.9362 \t Acc 63.97 \t AccHead 67.26 \t AccTail 51.42\n",
      "Epoch: [015] \t Loss 0.8974 \t Acc 67.16 \t AccHead 74.18 \t AccTail 40.33\n",
      "Epoch: [016] \t Loss 0.8724 \t Acc 65.83 \t AccHead 71.59 \t AccTail 43.87\n",
      "Epoch: [017] \t Loss 0.8569 \t Acc 70.44 \t AccHead 74.61 \t AccTail 54.48\n",
      "Epoch: [018] \t Loss 0.8196 \t Acc 68.48 \t AccHead 73.75 \t AccTail 48.35\n",
      "Epoch: [019] \t Loss 0.8132 \t Acc 69.60 \t AccHead 76.84 \t AccTail 41.98\n",
      "Epoch: [020] \t Loss 0.7855 \t Acc 68.97 \t AccHead 73.32 \t AccTail 52.36\n",
      "Epoch: [021] \t Loss 0.7701 \t Acc 71.46 \t AccHead 76.59 \t AccTail 51.89\n",
      "Epoch: [022] \t Loss 0.7440 \t Acc 70.00 \t AccHead 76.53 \t AccTail 45.05\n",
      "Epoch: [023] \t Loss 0.7358 \t Acc 71.07 \t AccHead 77.02 \t AccTail 48.35\n",
      "Epoch: [024] \t Loss 0.7234 \t Acc 72.30 \t AccHead 76.22 \t AccTail 57.31\n",
      "Epoch: [025] \t Loss 0.6990 \t Acc 72.34 \t AccHead 75.66 \t AccTail 59.67\n",
      "Epoch: [026] \t Loss 0.6850 \t Acc 73.08 \t AccHead 79.18 \t AccTail 49.76\n",
      "Epoch: [027] \t Loss 0.6859 \t Acc 75.23 \t AccHead 80.36 \t AccTail 55.66\n",
      "Epoch: [028] \t Loss 0.6545 \t Acc 73.67 \t AccHead 78.44 \t AccTail 55.42\n",
      "Epoch: [029] \t Loss 0.6549 \t Acc 74.65 \t AccHead 78.57 \t AccTail 59.67\n",
      "Epoch: [030] \t Loss 0.6418 \t Acc 72.10 \t AccHead 77.70 \t AccTail 50.71\n",
      "Epoch: [031] \t Loss 0.6349 \t Acc 74.84 \t AccHead 79.25 \t AccTail 58.02\n",
      "Epoch: [032] \t Loss 0.6199 \t Acc 75.38 \t AccHead 80.98 \t AccTail 54.01\n",
      "Epoch: [033] \t Loss 0.6018 \t Acc 73.91 \t AccHead 78.69 \t AccTail 55.66\n",
      "Epoch: [034] \t Loss 0.5994 \t Acc 75.97 \t AccHead 80.30 \t AccTail 59.43\n",
      "Epoch: [035] \t Loss 0.5827 \t Acc 75.53 \t AccHead 79.93 \t AccTail 58.73\n",
      "Epoch: [036] \t Loss 0.5802 \t Acc 75.72 \t AccHead 80.61 \t AccTail 57.08\n",
      "Epoch: [037] \t Loss 0.5725 \t Acc 75.13 \t AccHead 79.18 \t AccTail 59.67\n",
      "Epoch: [038] \t Loss 0.5693 \t Acc 74.69 \t AccHead 82.52 \t AccTail 44.81\n",
      "Epoch: [039] \t Loss 0.5536 \t Acc 74.45 \t AccHead 80.11 \t AccTail 52.83\n",
      "Epoch: [040] \t Loss 0.5585 \t Acc 77.39 \t AccHead 84.19 \t AccTail 51.42\n",
      "Epoch: [041] \t Loss 0.5589 \t Acc 76.99 \t AccHead 80.36 \t AccTail 64.15\n",
      "Epoch: [042] \t Loss 0.5399 \t Acc 74.35 \t AccHead 78.51 \t AccTail 58.49\n",
      "Epoch: [043] \t Loss 0.5299 \t Acc 75.97 \t AccHead 80.67 \t AccTail 58.02\n",
      "Epoch: [044] \t Loss 0.5223 \t Acc 76.85 \t AccHead 80.42 \t AccTail 63.21\n",
      "Epoch: [045] \t Loss 0.5152 \t Acc 76.70 \t AccHead 81.35 \t AccTail 58.96\n",
      "Epoch: [046] \t Loss 0.5061 \t Acc 77.04 \t AccHead 82.40 \t AccTail 56.60\n",
      "Epoch: [047] \t Loss 0.4996 \t Acc 76.16 \t AccHead 81.66 \t AccTail 55.19\n",
      "Epoch: [048] \t Loss 0.5026 \t Acc 76.06 \t AccHead 78.32 \t AccTail 67.45\n",
      "Epoch: [049] \t Loss 0.4931 \t Acc 76.06 \t AccHead 80.17 \t AccTail 60.38\n",
      "Epoch: [050] \t Loss 0.4955 \t Acc 76.51 \t AccHead 81.53 \t AccTail 57.31\n",
      "Epoch: [051] \t Loss 0.4883 \t Acc 75.72 \t AccHead 79.74 \t AccTail 60.38\n",
      "Epoch: [052] \t Loss 0.4624 \t Acc 76.80 \t AccHead 81.96 \t AccTail 57.08\n",
      "Epoch: [053] \t Loss 0.4655 \t Acc 74.99 \t AccHead 78.88 \t AccTail 60.14\n",
      "Epoch: [054] \t Loss 0.4849 \t Acc 77.04 \t AccHead 83.08 \t AccTail 54.01\n",
      "Epoch: [055] \t Loss 0.4643 \t Acc 76.11 \t AccHead 79.31 \t AccTail 63.92\n",
      "Epoch: [056] \t Loss 0.4599 \t Acc 76.70 \t AccHead 80.30 \t AccTail 62.97\n",
      "Epoch: [057] \t Loss 0.4400 \t Acc 76.95 \t AccHead 82.40 \t AccTail 56.13\n",
      "Epoch: [058] \t Loss 0.4539 \t Acc 78.27 \t AccHead 84.06 \t AccTail 56.13\n",
      "Epoch: [059] \t Loss 0.4458 \t Acc 76.16 \t AccHead 80.67 \t AccTail 58.96\n",
      "Epoch: [060] \t Loss 0.4359 \t Acc 77.92 \t AccHead 81.22 \t AccTail 65.33\n",
      "Epoch: [061] \t Loss 0.4424 \t Acc 75.23 \t AccHead 78.75 \t AccTail 61.79\n",
      "Epoch: [062] \t Loss 0.4449 \t Acc 77.78 \t AccHead 84.50 \t AccTail 52.12\n",
      "Epoch: [063] \t Loss 0.4299 \t Acc 78.76 \t AccHead 82.77 \t AccTail 63.44\n",
      "Epoch: [064] \t Loss 0.4203 \t Acc 77.09 \t AccHead 79.74 \t AccTail 66.98\n",
      "Epoch: [065] \t Loss 0.4296 \t Acc 77.14 \t AccHead 82.64 \t AccTail 56.13\n",
      "Epoch: [066] \t Loss 0.4135 \t Acc 77.58 \t AccHead 78.88 \t AccTail 72.64\n",
      "Epoch: [067] \t Loss 0.4089 \t Acc 78.17 \t AccHead 82.40 \t AccTail 62.03\n",
      "Epoch: [068] \t Loss 0.4243 \t Acc 79.25 \t AccHead 84.56 \t AccTail 58.96\n",
      "Epoch: [069] \t Loss 0.4080 \t Acc 78.12 \t AccHead 82.27 \t AccTail 62.26\n",
      "Epoch: [070] \t Loss 0.4233 \t Acc 75.92 \t AccHead 82.52 \t AccTail 50.71\n",
      "Epoch: [071] \t Loss 0.4018 \t Acc 76.46 \t AccHead 80.85 \t AccTail 59.67\n",
      "Epoch: [072] \t Loss 0.3993 \t Acc 75.38 \t AccHead 80.23 \t AccTail 56.84\n",
      "Epoch: [073] \t Loss 0.3980 \t Acc 77.09 \t AccHead 81.90 \t AccTail 58.73\n",
      "Epoch: [074] \t Loss 0.4000 \t Acc 76.95 \t AccHead 80.48 \t AccTail 63.44\n",
      "Epoch: [075] \t Loss 0.4064 \t Acc 77.39 \t AccHead 80.48 \t AccTail 65.57\n",
      "Epoch: [076] \t Loss 0.3796 \t Acc 77.24 \t AccHead 79.80 \t AccTail 67.45\n",
      "Epoch: [077] \t Loss 0.3819 \t Acc 79.69 \t AccHead 82.46 \t AccTail 69.10\n",
      "Epoch: [078] \t Loss 0.3840 \t Acc 77.09 \t AccHead 82.03 \t AccTail 58.25\n",
      "Epoch: [079] \t Loss 0.3829 \t Acc 76.85 \t AccHead 82.27 \t AccTail 56.13\n",
      "Epoch: [080] \t Loss 0.3935 \t Acc 77.19 \t AccHead 80.42 \t AccTail 64.86\n",
      "Epoch: [081] \t Loss 0.3707 \t Acc 77.63 \t AccHead 82.95 \t AccTail 57.31\n",
      "Epoch: [082] \t Loss 0.3666 \t Acc 76.51 \t AccHead 81.41 \t AccTail 57.78\n",
      "Epoch: [083] \t Loss 0.3703 \t Acc 77.78 \t AccHead 79.56 \t AccTail 70.99\n",
      "Epoch: [084] \t Loss 0.3653 \t Acc 76.65 \t AccHead 80.23 \t AccTail 62.97\n",
      "Epoch: [085] \t Loss 0.3833 \t Acc 78.61 \t AccHead 83.57 \t AccTail 59.67\n",
      "Epoch: [086] \t Loss 0.3480 \t Acc 79.15 \t AccHead 84.25 \t AccTail 59.67\n",
      "Epoch: [087] \t Loss 0.3672 \t Acc 78.66 \t AccHead 83.38 \t AccTail 60.61\n",
      "Epoch: [088] \t Loss 0.3668 \t Acc 77.63 \t AccHead 81.66 \t AccTail 62.26\n",
      "Epoch: [089] \t Loss 0.3540 \t Acc 77.58 \t AccHead 81.47 \t AccTail 62.74\n",
      "Epoch: [090] \t Loss 0.3613 \t Acc 77.53 \t AccHead 81.90 \t AccTail 60.85\n",
      "Epoch: [091] \t Loss 0.3622 \t Acc 77.78 \t AccHead 82.58 \t AccTail 59.43\n",
      "Epoch: [092] \t Loss 0.3615 \t Acc 77.29 \t AccHead 81.28 \t AccTail 62.03\n",
      "Epoch: [093] \t Loss 0.3586 \t Acc 79.25 \t AccHead 84.13 \t AccTail 60.61\n",
      "Epoch: [094] \t Loss 0.3477 \t Acc 79.05 \t AccHead 82.95 \t AccTail 64.15\n",
      "Epoch: [095] \t Loss 0.3468 \t Acc 77.04 \t AccHead 78.44 \t AccTail 71.70\n",
      "Epoch: [096] \t Loss 0.3401 \t Acc 78.90 \t AccHead 81.90 \t AccTail 67.45\n",
      "Epoch: [097] \t Loss 0.3487 \t Acc 77.73 \t AccHead 82.83 \t AccTail 58.25\n",
      "Epoch: [098] \t Loss 0.3366 \t Acc 79.30 \t AccHead 85.30 \t AccTail 56.37\n",
      "Epoch: [099] \t Loss 0.3363 \t Acc 78.51 \t AccHead 81.41 \t AccTail 67.45\n",
      "Epoch: [100] \t Loss 0.3342 \t Acc 78.61 \t AccHead 83.32 \t AccTail 60.61\n",
      "1163.9623489379883\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "train_start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_history = []\n",
    "    model.train()\n",
    "    for batch_index, data in enumerate(train_loader):\n",
    "        image, target = data\n",
    "        image, target = image.cuda(), target.cuda()\n",
    "\n",
    "        pred = model(image)\n",
    "        loss = criterion(pred, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "    topk_acc, head_acc, tail_acc = compute_accuracy(val_loader, model)\n",
    "    loss_mean = np.mean(loss_history)\n",
    "    scheduler.step()\n",
    "\n",
    "    print('Epoch: [{:03d}] \\t Loss {:.4f} \\t Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(epoch+1, loss_mean, topk_acc[0], head_acc[0], tail_acc[0]))\n",
    "\n",
    "train_end_time = time.time()\n",
    "\n",
    "print(train_end_time-train_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28899fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 69.51 \t AccHead 79.24 \t AccTail 59.78\n",
      "4.727281332015991\n"
     ]
    }
   ],
   "source": [
    "train_start_time = time.time()\n",
    "\n",
    "topk_acc, head_acc, tail_acc = compute_accuracy(test_loader, model)\n",
    "\n",
    "print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc[0], head_acc[0], tail_acc[0]))\n",
    "\n",
    "train_end_time = time.time()\n",
    "\n",
    "print(train_end_time-train_start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
